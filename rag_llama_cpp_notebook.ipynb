{"cells":[{"cell_type":"markdown","id":"3d167882","metadata":{"id":"3d167882"},"source":["# RAG with LLaMA.cpp Tutorial"]},{"cell_type":"markdown","id":"1842ea90","metadata":{"id":"1842ea90"},"source":["# Introduction: Why RAG with LLMs Matters"]},{"cell_type":"markdown","id":"67df2796","metadata":{"id":"67df2796"},"source":["\n","In the age of large language models (LLMs), one critical limitation remains: their knowledge is frozen at the time of training. **Retrieval-Augmented Generation (RAG)** solves this problem by allowing LLMs to dynamically fetch up-to-date, domain-specific, or private information from external sources like documents or databases.\n","\n","This approach enhances accuracy, reduces hallucinations, and enables real-time applications in business, education, research, and more. By combining fast local inference via `llama-cpp-python` with smart document retrieval, we create powerful and private AI systems that go far beyond traditional chatbots."]},{"cell_type":"markdown","id":"8c78a991","metadata":{"id":"8c78a991"},"source":["## Step 1: Install Required Libraries"]},{"cell_type":"code","execution_count":null,"id":"1f5dbc37","metadata":{"id":"1f5dbc37"},"outputs":[],"source":["!pip install llama-cpp-python chromadb tiktoken huggingface_hub sentence_transformers"]},{"cell_type":"markdown","id":"84d14a16","metadata":{"id":"84d14a16"},"source":["Step 2: Import Required Modules"]},{"cell_type":"code","execution_count":null,"id":"8a3aff28","metadata":{"id":"8a3aff28"},"outputs":[],"source":["from llama_cpp import Llama\n","import chromadb\n","from chromadb.config import Settings\n","from sentence_transformers import SentenceTransformer\n","from chromadb.utils.embedding_functions import EmbeddingFunction\n","import os\n","import time"]},{"cell_type":"markdown","id":"c6ab849e","metadata":{"id":"c6ab849e"},"source":["\n","- `llama_cpp.Llama`: This is the Python binding to the LLaMA C++ inference engine, used for running a local LLM.\n","- `chromadb`: A simple and fast embedding-based vector store used here for document retrieval.\n","- `OpenAIEmbeddingFunction`: Used to create embeddings for the documents; can be replaced with other providers."]},{"cell_type":"markdown","id":"d2073029","metadata":{"id":"d2073029"},"source":["Step 3: Set Up Chroma Vector Store"]},{"cell_type":"code","execution_count":null,"id":"e8fec0cd","metadata":{"id":"e8fec0cd"},"outputs":[],"source":["token = \"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\" #@param {type:\"string\"}\n","os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = token\n","\n","# Recommended way to initialize a persistent Chroma client\n","chroma_client = chromadb.PersistentClient(path=\".chroma\")\n","\n","# Load a small, fast, local model\n","local_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n","\n","# Wrap it in ChromaDB's expected interface\n","class LocalEmbeddingFunction(EmbeddingFunction):\n","    def __call__(self, texts):\n","        return local_model.encode(texts).tolist()\n","\n","embedding_func = LocalEmbeddingFunction()\n","\n","# Create or get the collection\n","collection = chroma_client.get_or_create_collection(\n","    name=\"rag-tutorial\",\n","    embedding_function=embedding_func\n",")"]},{"cell_type":"markdown","id":"43f43907","metadata":{"id":"43f43907"},"source":["\n","- The vector store is initialized using DuckDB as the backend.\n","- A collection is created (or retrieved if it already exists) where document embeddings will be stored.\n","- `text-embedding-ada-002` is a performant embedding model from OpenAI."]},{"cell_type":"markdown","id":"5fc31ac3","metadata":{"id":"5fc31ac3"},"source":["Step 4: Ingest Documents into the Vector Store"]},{"cell_type":"code","execution_count":null,"id":"57445e92","metadata":{"id":"57445e92"},"outputs":[],"source":["documents = [\n","    \"The capital of France is Paris.\",\n","    \"The Moon is Earth's only natural satellite.\",\n","    \"Python is a widely used programming language for machine learning.\"\n","]\n","collection.add(documents=documents, ids=[\"doc1\", \"doc2\", \"doc3\"])"]},{"cell_type":"markdown","id":"e2916b98","metadata":{"id":"e2916b98"},"source":["\n","Here we add 3 simple documents into the vector store with IDs for later retrieval.\n","In a real application, you would preprocess and chunk PDFs, HTML, etc."]},{"cell_type":"markdown","id":"90ed4779","metadata":{"id":"90ed4779"},"source":["Step 5: Define a Function to Retrieve Relevant Context from Chroma"]},{"cell_type":"code","execution_count":null,"id":"660b6711","metadata":{"id":"660b6711"},"outputs":[],"source":["def get_context(query, top_k=2):\n","    results = collection.query(query_texts=[query], n_results=top_k)\n","    return \"\\n\".join(results[\"documents\"][0])"]},{"cell_type":"markdown","id":"e296531b","metadata":{"id":"e296531b"},"source":["\n","This function uses ChromaDB to retrieve the top `k` most relevant documents to the user's query.\n","We return them concatenated as context."]},{"cell_type":"markdown","id":"081bfbae","metadata":{"id":"081bfbae"},"source":["Step 6: Load LLaMA Model Locally with llama-cpp-python"]},{"cell_type":"code","execution_count":null,"id":"aed16981","metadata":{"id":"aed16981"},"outputs":[],"source":["from huggingface_hub import hf_hub_download\n","\n","# TinyLlama requires no authentication or approval\n","# Replace with desired repo and filename\n","model_path = hf_hub_download(\n","    repo_id=\"TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF\",\n","    filename=\"tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf\",\n","    local_dir=\"./models\",\n","    local_dir_use_symlinks=False\n",")\n","print(\"Downloaded model path:\", model_path)\n","\n","# RAM requirements: 8GB, overkill\n","# llm = Llama(model_path=\"./models/llama-2-7b-chat.ggmlv3.q4_0.bin\", n_ctx=2048)\n","\n","# smaller model, quantized version 2-3GB, powerful desktop\n","#llm = Llama(model_path=\"./models/llama-2-7b-chat.Q3_K_S.gguf\", n_ctx=1024)\n","\n","# Phi-2 model, 1.7B parameter, teaching code logic, 2.3GB\n","#llm = Llama(model_path=\"./models/phi-2.Q4_K_M.gguf\", n_ctx=512)\n","\n","# TinyLlama, 1.1B parameter, classroom demo, 1.3GB\n","llm = Llama(model_path=\"./models/tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf\", n_ctx=512)\n"]},{"cell_type":"markdown","id":"8838165a","metadata":{"id":"8838165a"},"source":["\n","Loads the quantized model file (in ggml format). Make sure it's compatible with llama.cpp.\n","`n_ctx` is the maximum token context size."]},{"cell_type":"markdown","id":"c83e5829","metadata":{"id":"c83e5829"},"source":["Step 7: Define a Prompt Template and Generate Answer"]},{"cell_type":"code","execution_count":null,"id":"d5846d81","metadata":{"id":"d5846d81"},"outputs":[],"source":["def rag_prompt(query, context):\n","    return f\"\"\"\n","    Answer the question using only the context below.\n","    Context:\n","    {context}\n","\n","    Question: {query}\n","    Answer:\"\"\"\n"]},{"cell_type":"markdown","id":"a4933904","metadata":{"id":"a4933904"},"source":["- `rag_prompt()` creates a structured prompt where the LLM is instructed to use only the given context.\n","- `generate_answer()` performs the full RAG pipeline: retrieval, prompt construction, and generation."]},{"cell_type":"markdown","id":"bda53eec","metadata":{"id":"bda53eec"},"source":["# Step 8: Try an Example Query"]},{"cell_type":"markdown","source":["Here's a classroom-robust version of generate_answer(), designed to be:\n","\n","- Readable for students\n","\n","- Transparent (shows what's happening under the hood)\n","\n","- Safe from weird LLM behaviors (like answering more than one question)\n","\n","- Easy to debug and extend\n","\n","- With Logging and Safe Stops"],"metadata":{"id":"fClVv0EMjtzB"},"id":"fClVv0EMjtzB"},{"cell_type":"code","source":["def generate_answer(query, top_k=2, max_tokens=128):\n","    \"\"\"\n","    Generate an answer using the RAG pipeline with logging and safety.\n","\n","    Parameters:\n","    - query: the question to answer\n","    - top_k: how many documents to retrieve from ChromaDB\n","    - max_tokens: how many tokens to generate in the response\n","\n","    Returns:\n","    - A string containing the model's answer\n","    \"\"\"\n","\n","    # Retrieve context\n","    context = get_context(query, top_k=top_k)\n","    print(\"\\n--- Retrieved Context ---\")\n","    print(context)\n","\n","    # Construct prompt\n","    prompt = f\"\"\"You are a helpful assistant. Use the context below to answer the question concisely.\n","\n","Context:\n","{context}\n","\n","Question: {query}\n","Answer:\"\"\"\n","    print(\"\\n--- Prompt Sent to Model ---\")\n","    print(prompt)\n","\n","    # Generate model output\n","    output = llm(prompt, max_tokens=max_tokens, stop=[\"\\nQ:\", \"\\nQuestion:\", \"\\n###\"])\n","\n","    # Log full output structure\n","    print(\"\\n--- Raw Model Output ---\")\n","    print(output)\n","\n","    # Extract text and clean it\n","    answer = output[\"choices\"][0][\"text\"].strip()\n","\n","    print(\"\\n--- Final Answer ---\")\n","    print(answer)\n","\n","    return answer\n"],"metadata":{"id":"I4wJHbN-jsvG"},"id":"I4wJHbN-jsvG","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"68f9ed79","metadata":{"id":"68f9ed79"},"outputs":[],"source":["\n","query = \"What is the capital of France?\"\n","response = generate_answer(query)\n","print(\"Response:\", response)"]},{"cell_type":"markdown","id":"2b828614","metadata":{"id":"2b828614"},"source":["# Conclusion"]},{"cell_type":"markdown","id":"a7b65aad","metadata":{"id":"a7b65aad"},"source":["With just a few components—embedding function, vector store, and a local LLM—you now have a working RAG pipeline.\n","This pattern is highly extensible: you can swap in your own PDFs, use local embedding models, fine-tune the prompt, or even stream results in real time."]},{"cell_type":"markdown","id":"70e55e49","metadata":{"id":"70e55e49"},"source":["In future applications, RAG will power:\n","- Personal assistants with access to your private notes and documents\n","- Customer support bots with up-to-date knowledge bases\n","- Academic or enterprise search engines\n","- Local-first and privacy-preserving AI applications\n","\n","### Students who master RAG will be equipped to build the next generation of intelligent, context-aware systems.\n"]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}