{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aircable/AInotebooks/blob/main/NN_with_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee4B4v5tAp1C"
      },
      "source": [
        "# A Simple Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4cEhtf_Ap1E"
      },
      "source": [
        "In this tutorial we will implement a simple neural network from scratch using PyTorch. The idea of the tutorial is to teach you the basics of PyTorch and how it can be used to implement a neural network from scratch. I will go over some of the basic functionalities and concepts available in PyTorch that will allow you to build your own neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP9ewMSlC7JU"
      },
      "source": [
        "\n",
        "The `torch` module provides all the necessary **tensor** operators you will need to implement your first neural network from scratch in PyTorch. In PyTorch everything is a Tensor, so this is the first thing you will need to get used to. Let's import the libraries we will need for this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKmXKSQnAp1G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EWBBl1nAp1M"
      },
      "source": [
        "## Data\n",
        "Let's start by creating some sample data using the `torch.tensor` command. In Numpy, this could be done with `np.array`. Both functions serve the same purpose, but in PyTorch everything is a Tensor as opposed to a vector or matrix. We define types in PyTorch using the `dtype=torch.xxx` command.\n",
        "\n",
        "In the data below, `X` represents the amount of hours studied and how much time students spent sleeping, whereas `y` represent grades. The variable `xPredicted` is a single input for which we want to predict a grade using the parameters learned by the neural network. Remember, the neural network wants to learn a mapping between `X` and `y`, so it will try to take a guess from what it has learned from the training data.\n",
        "\n",
        "<img src=\"input_table.jpg\" alt=\"input table\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsAVbHnjAp1P"
      },
      "outputs": [],
      "source": [
        "X = torch.tensor(([2, 9], [1, 5], [3, 6]), dtype=torch.float) # 3 X 2 tensor\n",
        "y = torch.tensor(([92], [100], [89]), dtype=torch.float) # 3 X 1 tensor\n",
        "xPredicted = torch.tensor(([4, 8]), dtype=torch.float) # 1 X 2 tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC0ru9kCAp1U"
      },
      "source": [
        "You can check the size of the tensors we have just created with the `size` command. This is equivalent to the `shape` command used in tools such as Numpy and Tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "sfC-B1BEAp1W",
        "outputId": "eba8424c-519d-48f7-ccd7-bc6a76035c61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 2])\n",
            "torch.Size([3, 1])\n"
          ]
        }
      ],
      "source": [
        "print(X.size())\n",
        "print(y.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrND9MS9Ap1f"
      },
      "source": [
        "## Scaling\n",
        "\n",
        "Below we are performing some scaling on the sample data. Notice that the `max` function returns both a tensor and the corresponding indices. So we use `_` to capture the indices which we won't use here because we are only interested in the max values to conduct the scaling. Perfect! Our data is now in a very nice format our neural network will appreciate later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "hlBvtfAmAp1i",
        "outputId": "82145a5e-d662-44b8-c8ce-e80f9a381694"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.5000, 1.0000])\n"
          ]
        }
      ],
      "source": [
        "# scale units\n",
        "X_max, _ = torch.max(X, 0)\n",
        "xPredicted_max, _ = torch.max(xPredicted, 0)\n",
        "\n",
        "X = torch.div(X, X_max)\n",
        "xPredicted = torch.div(xPredicted, xPredicted_max)\n",
        "y = y / 100  # max test score is 100\n",
        "print(xPredicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1kTs5S5Ap1m"
      },
      "source": [
        "Notice that there are two functions `max` and `div` that I didn't discuss above. They do exactly what they imply: `max` finds the maximum value in a vector... I mean tensor; and `div` is basically a nice little function to divide two tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRvMSpEFAp1n"
      },
      "source": [
        "## Model (Computation Graph)\n",
        "Once the data has been processed and it is in the proper format, all you need to do now is to define your model. Here is where things begin to change a little as compared to how you would build your neural networks using, say, something like Keras or Tensorflow. However, you will realize quickly as you go along that PyTorch doesn't differ much from other deep learning tools. At the end of the day we are constructing a computation graph, which is used to dictate how data should flow and what type of operations are performed on this information.\n",
        "\n",
        "For illustration purposes, we are building the following neural network or computation graph:\n",
        "\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1l-sKpcCJCEUJV1BlAqcVAvLXLpYCInV6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7pDC5SfAp1p"
      },
      "outputs": [],
      "source": [
        "class Neural_Network(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(Neural_Network, self).__init__()\n",
        "        # parameters\n",
        "        # TODO: parameters can be parameterized instead of declaring them here\n",
        "        self.inputSize = 2\n",
        "        self.outputSize = 1\n",
        "        self.hiddenSize = 3\n",
        "\n",
        "        # weights\n",
        "        self.W1 = torch.randn(self.inputSize, self.hiddenSize) # 3 X 2 tensor\n",
        "        self.W2 = torch.randn(self.hiddenSize, self.outputSize) # 3 X 1 tensor\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.z = torch.matmul(X, self.W1) # 3 X 3 \".dot\" does not broadcast in PyTorch\n",
        "        self.z2 = self.sigmoid(self.z) # activation function\n",
        "        self.z3 = torch.matmul(self.z2, self.W2)\n",
        "        o = self.sigmoid(self.z3) # final activation function\n",
        "        return o\n",
        "\n",
        "    def sigmoid(self, s):\n",
        "        return 1 / (1 + torch.exp(-s))\n",
        "\n",
        "    def sigmoidPrime(self, s):\n",
        "        # derivative of sigmoid\n",
        "        return s * (1 - s)\n",
        "\n",
        "    def backward(self, X, y, o):\n",
        "        self.o_error = y - o # error in output\n",
        "        self.o_delta = self.o_error * self.sigmoidPrime(o) # derivative of sig to error\n",
        "        self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2)) # z2 error: how much our hidden layer weights contributed to output error  \n",
        "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2) # applying derivative of sigmoid to z2 error  \n",
        "        self.W1 += torch.matmul(torch.t(X), self.z2_delta) # adjusting first set (input --> hidden) weights \n",
        "        self.W2 += torch.matmul(torch.t(self.z2), self.o_delta) # adjusting second set (hidden --> output) weights\n",
        "        # Note: in a real-world scenario, you would use an optimizer to update weights\n",
        "\n",
        "    def train(self, X, y):\n",
        "        # forward + backward pass for training\n",
        "        o = self.forward(X)\n",
        "        self.backward(X, y, o)\n",
        "\n",
        "    def saveWeights(self, model):\n",
        "        # we will use the PyTorch internal storage functions\n",
        "        torch.save(model, \"NN\")\n",
        "        # you can reload model with all the weights and so forth with:\n",
        "        # torch.load(\"NN\")\n",
        "\n",
        "    def predict(self):\n",
        "        print (\"Predicted data based on trained weights: \")\n",
        "        print (\"Input (scaled): \\n\" + str(xPredicted))\n",
        "        print (\"Output: \\n\" + str(self.forward(xPredicted)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm5gimnyAp1s"
      },
      "source": [
        "For the purpose of this tutorial, we are not going to be talking math stuff, that's for another day. I just want you to get a gist of what it takes to build a neural network from scratch using PyTorch. Let's break down the model which was declared via the class above.\n",
        "\n",
        "## Class Header\n",
        "First, we defined our model via a class because that is the recommended way to build the computation graph. The class header contains the name of the class `Neural Network` and the parameter `nn.Module` which basically indicates that we are defining our own neural network.\n",
        "\n",
        "```python\n",
        "class Neural_Network(nn.Module):\n",
        "```\n",
        "\n",
        "## Initialization\n",
        "The next step is to define the initializations ( `def __init__(self,)`) that will be performed upon creating an instance of the customized neural network. You can declare the parameters of your model here, but typically, you would declare the structure of your network in this section -- the size of the hidden layers and so forth. Since we are building the neural network from scratch, we explicitly declared the size of the weights matrices: one that stores the parameters from the input to hidden layer; and one that stores the parameter from the hidden to output layer. Both weight matrices are initialized with values randomly chosen from a normal distribution via `torch.randn(...)`. Note that we are not using bias just to keep things as simple as possible.  \n",
        "\n",
        "```python\n",
        "def __init__(self, ):\n",
        "    super(Neural_Network, self).__init__()\n",
        "    # parameters\n",
        "    # TODO: parameters can be parameterized instead of declaring them here\n",
        "    self.inputSize = 2\n",
        "    self.outputSize = 1\n",
        "    self.hiddenSize = 3\n",
        "\n",
        "    # weights\n",
        "    self.W1 = torch.randn(self.inputSize, self.hiddenSize) # 3 X 2 tensor\n",
        "    self.W2 = torch.randn(self.hiddenSize, self.outputSize) # 3 X 1 tensor\n",
        "```\n",
        "\n",
        "## The Forward Function\n",
        "The `forward` function is where all the magic happens (see below). This is where the data enters and is fed into the computation graph (i.e., the neural network structure we have built). Since we are building a simple neural network with one hidden layer, our forward function looks very simple:\n",
        "\n",
        "```python\n",
        "def forward(self, X):\n",
        "    self.z = torch.matmul(X, self.W1)\n",
        "    self.z2 = self.sigmoid(self.z) # activation function\n",
        "    self.z3 = torch.matmul(self.z2, self.W2)\n",
        "    o = self.sigmoid(self.z3) # final activation function\n",
        "    return o\n",
        "```\n",
        "\n",
        "The `forward` function above takes the input `X`and then performs a matrix multiplication (`torch.matmul(...)`) with the first weight matrix `self.W1`. Then the result is applied an activation function, `sigmoid`. The resulting matrix of the activation is then multiplied with the second weight matrix `self.W2`. Then another activation if performed, which renders the output of the neural network or computation graph. The process I described above is simply what's known as a `feedforward pass`. In order for the weights to optimize when training, we need a backpropagation algorithm.\n",
        "\n",
        "## The Backward Function\n",
        "The `backward` function contains the backpropagation algorithm, where the goal is to essentially minimize the loss with respect to our weights. In other words, the weights need to be updated in such  a way that the loss decreases while the neural network is training (well, that is what we hope for). All this magic is possible with the gradient descent algorithm which is declared in the `backward` function. Take a minute or two to inspect what is happening in the code below:\n",
        "\n",
        "```python\n",
        "def backward(self, X, y, o):\n",
        "    self.o_error = y - o # error in output\n",
        "    self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
        "    self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2))\n",
        "    self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)\n",
        "    self.W1 += torch.matmul(torch.t(X), self.z2_delta)\n",
        "    self.W2 += torch.matmul(torch.t(self.z2), self.o_delta)\n",
        "```\n",
        "\n",
        "Notice that we are performing a lot of matrix multiplications along with the transpose operations via the `torch.matmul(...)` and `torch.t(...)` operations, respectively. The rest is simply gradient descent -- there is nothing to it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t26Dr5zAp1u"
      },
      "source": [
        "## Training\n",
        "All that is left now is to train the neural network. First we create an instance of the computation graph we have just built:\n",
        "\n",
        "```python\n",
        "NN = Neural_Network()\n",
        "```\n",
        "\n",
        "Then we train the model for `1000` rounds. Notice that in PyTorch `NN(X)` automatically calls the `forward` function so there is no need to explicitly call `NN.forward(X)`.\n",
        "\n",
        "After we have obtained the predicted output for ever round of training, we compute the loss, with the following code:\n",
        "\n",
        "```python\n",
        "    torch.mean((y - NN(X))**2).detach().item()\n",
        "```\n",
        "\n",
        "The next step is to start the training (foward + backward) via `NN.train(X, y)`. After we have trained the neural network, we can store the model and output the predicted value of the single instance we declared in the beginning, `xPredicted`.  \n",
        "\n",
        "### Let's train!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's add a real time graph for how your neural network learns. It feels like magic sometimes, but at the end of the day, any neural network is simply trying to get loss to 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in /home/juergen/miniconda3/lib/python3.12/site-packages (3.10.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/juergen/miniconda3/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/juergen/miniconda3/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/juergen/miniconda3/lib/python3.12/site-packages (from matplotlib) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/juergen/miniconda3/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /home/juergen/miniconda3/lib/python3.12/site-packages (from matplotlib) (2.1.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/juergen/miniconda3/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=8 in /home/juergen/miniconda3/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/juergen/miniconda3/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/juergen/miniconda3/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /home/juergen/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# for plotting we may need matplotlib\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "9sTddOpLAp1w",
        "outputId": "1c1beaf2-ace7-4ac1-c2f9-eb943c29c1f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#0 Loss: 0.24544493854045868\n",
            "#100 Loss: 0.0026628002524375916\n",
            "#200 Loss: 0.0024748605210334063\n",
            "#300 Loss: 0.002363199135288596\n",
            "#400 Loss: 0.0022466194350272417\n",
            "#500 Loss: 0.0021235516760498285\n",
            "#600 Loss: 0.001996910898014903\n",
            "#700 Loss: 0.0018705682596191764\n",
            "#800 Loss: 0.0017485078424215317\n",
            "#900 Loss: 0.0016340742586180568\n",
            "Predicted data based on trained weights: \n",
            "Input (scaled): \n",
            "tensor([0.5000, 1.0000])\n",
            "Output: \n",
            "tensor([0.9529])\n",
            "Finished training!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Neural_Network. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "count = [] # list to store iteration count\n",
        "loss = [] # list to store loss values\n",
        "\n",
        "NN = Neural_Network()\n",
        "for i in range(1000):  # trains the NN 1,000 times\n",
        "    # real time graph how the NN is learning\n",
        "    print(\"# \" + str(i) + \"\\n\")\n",
        "    print(\"Input (scaled): \\n\" + str(X))\n",
        "    print(\"Actual Output: \\n\" + str(y))\n",
        "    print(\"Predicted Output: \\n\" + str(nn.forward(X)))\n",
        "    loss = str(np.mean(np.square(y - nn.forward(X))))\n",
        "    print(\"Loss: \\n\" +  loss ) # mean squared error\n",
        "    print(\"\\n\")\n",
        "    count.append(i)\n",
        "    loss.append(np.round(float(loss), 6))\n",
        "    plt.cla()\n",
        "    plt.title(\"Loss over Iterations\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.plot(count, loss)\n",
        "    plt.pause(.001)\n",
        "      \n",
        "    if (i % 100) == 0:\n",
        "        print (\"#\" + str(i) + \" Loss: \" + str(torch.mean((y - NN(X))**2).detach().item()))  # mean sum squared loss\n",
        "    NN.train(X, y)\n",
        "NN.saveWeights(NN)\n",
        "NN.predict()\n",
        "\n",
        "print(\"Finished training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The loss keeps decreasing, which means that the neural network is learning something. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculations Behind Our Network Training: Forward Propagation\n",
        "We start with random numbers in our NN network.\n",
        "\n",
        "<img src=\"weights.png\"  width=\"25%\" alt=\"weights\">\n",
        "\n",
        "Our neural network can be represented with matrices. We take the dot product of each row of the first matrix and each column of the second matrix. What's the dot product? The dot product is the sum of the products of the elements. This is done element-wise. So, in order to get the first element in the hidden layer matrix, you multiply 2 by .2 and add that to 9 times .8 to get 7.6.\n",
        "\n",
        "<img src=\"matrix_multiplication.png\"  width=\"25%\" alt=\"matrix1\">\n",
        "\n",
        "Once you repeat this process for all the columns in the weights matrix, you get all the hidden layer neuron values which are shown in the hidden layer matrix on the right.\n",
        "\n",
        "```\n",
        "(2 * .2) + (9 * .8) = 7.6\n",
        "(2 * .6) + (9 * .3) = 3.9\n",
        "(2 * .1) + (9 * .7) = 6.5\n",
        "```\n",
        "\n",
        "This is the fundamental concept behind forward propagation. Now, let's put all the other inputs into the inputs matrix.\n",
        "\n",
        "<img src=\"matrix_multiplication_2.png\"  width=\"25%\" alt=\"matrix2\">\n",
        "\n",
        "As you might notice, every time we move down a row in the inputs, we move down a row in the result. \n",
        "\n",
        "The values we got in our hidden matrix are in the small font on the top left, why? Well, we must apply the **activation function** on each of these values. In an artificial neural network, an activation function of a neuron defines the output of the neuron given a set of inputs. In other words, activation functions give a network a sense of how activated a neuron is by mapping the input set to some value in between a given lower and upper bound. This is inspired by biology as certain Neural_Networkneurons within our brains are either firing or not depending on stimuli. You may think of a neuron firing as represented with a 1 and a neuron not firing as represented with a 0.\n",
        "\n",
        "There are many activation functions out there. In this case, we’ll stick to one of the more popular ones — the sigmoid function. The sigmoid function maps all input values to some value between a lower limit of 0 and an upper limit of 1. If the input is very negative, the number will be transformed into a number very close to 0. If the input is very positive, the number will be transformed to a number very close to 1. If the input is close to 0, the number will be transformed into some number in between 0 and 1.\n",
        "\n",
        "<img src=\"sigmoid.png\"  width=\"25%\" alt=\"weights\">\n",
        "\n",
        "```\n",
        "S(7.6) = 0.9994997\n",
        "S(3.9) = 0.9801597\n",
        "S(6.5) = 0.9984988\n",
        "```\n",
        "\n",
        "Now, we need to use matrix multiplication again, with another set of random weights, to calculate our output layer value.\n",
        "\n",
        "<img src=\"matrix_multiplication_3.png\"  width=\"25%\" alt=\"matrix2\">\n",
        "\n",
        "```\n",
        "(.9994 * .4) + (.9802 * .5) + (.9984 * .9) = 1.78842\n",
        "```\n",
        "\n",
        "Lastly, to normalize the output, we just apply the activation function again.\n",
        "\n",
        "```\n",
        "S(1.78842) = .8567335\n",
        "```\n",
        "\n",
        "And, there you go! We just did ***ONE** forward propagation! With those weights, our neural network will calculate .85 as our test score! However, our target was .92. Our result wasn’t poor, it just isn’t the best it can be. We just got a little lucky when we chose the random weights for this example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The “Learning” of Our Network: Backpropagation\n",
        "Since we have a random set of weights, we need to alter them to make our neural network guess the correct test scores. This is done through a method called backpropagation.\n",
        "\n",
        "Backpropagation works by using a loss function to calculate how far the network was from the target output.\n",
        "\n",
        "What does backpropagation adjust to make the neural network better at guessing the correct values?\n",
        "\n",
        "- The weights of the network.\n",
        "\n",
        "### Calculating Error\n",
        "One way of representing the loss (cost) function is by using the mean squared error function:\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (predictedOut - actualOutput)^2\n",
        "$$\n",
        "The mean squared error function is the sum, over all the data points, of the square of the difference between the predicted and actual target variables, divided by the number of data points. \n",
        "\n",
        "### Bias in Neural Networks\n",
        "In a neural network, we would ideally would want to have a bias. A bias allows us to shift the activation function either to the right or left, which means that we would be able to fit the prediction with the input data much better. The initial bias is dependent on your dataset, but this value should be updated along with the weights during backpropagation.\n",
        "\n",
        "For the sake of simplicity, we assume bias to be 0 in this tutorial.\n",
        "\n",
        "### Gradient Descent\n",
        "Now that we have the ****loss (cost) function**, our goal is to get it as close as we can to 0. As we are training our network, all we are doing is minimizing the loss function. In other words, we are optimizing to find the local minimum of our loss function.\n",
        "\n",
        "If you remember, our loss function is dependent on our output, which is dependent on our weights. We could brute force search all possible combinations of weights to minimize loss. However, this would take a really, really, really long time and just isn't practical. We need an intuitive method to find the local minimum of the loss function.\n",
        "\n",
        "For this optimization, we will use the method of gradient descent. Let's look at the function f(x), where x is our input weight and f(x) is our loss function. What if we could take the derivative at a given weight? This would allow us to understand which way is downhill and whether to make our x (weight) smaller or larger to decrease our loss. In other words, to figure out which direction to alter our weights, we need to find the rate of change of our loss with respect to our weights (a partial derivative!).\n",
        "\n",
        "If this partial derivative of our loss with respect to our weights is positive, then the cost function is going uphill. If it is negative, then the cost function is going downhill. Therefore, we're able to save time by making sure that we're optimizing in the right direction.\n",
        "\n",
        "<img src=\"gradient_descent.png\"  width=\"25%\" alt=\"weights\">\n",
        "\n",
        "Above is an illustration describing our method of gradient descent. By knowing which way to alter our weights, our outputs can only get more accurate.\n",
        "\n",
        "Here’s how we will calculate the incremental change to our weights:\n",
        "\n",
        "- Find the **margin of error** of the output layer (o) by taking the difference of the predicted output and the actual output (y)\n",
        "- Apply the derivative of our sigmoid activation function to the output layer error. We call this result the **delta output sum**.\n",
        "- Use the **delta output sum** of the output layer error to figure out how much our z2 (hidden) layer contributed to the output error by performing a dot product with our second weight matrix. We can call this the z2 error.\n",
        "- Calculate the **delta output sum** for the z2 layer by applying the derivative of our sigmoid **activation function** (just like step 2).\n",
        "- Adjust the weights for the first layer by performing a **dot product of the input layer** with the **hidden (z2) delta output sum**. For the second layer, perform a dot product of the hidden(z2) layer and the **output (o) delta output sum**.\n",
        "\n",
        "Calculating the delta output sum and then applying the derivative of the sigmoid function are very important to backpropagation. The derivative of the sigmoid, also known as **sigmoid prime**, will give us the rate of change, or slope, of the activation function at the output sum.\n",
        "\n",
        "We are adding a sigmoidPrime (derivative of sigmoid) function:\n",
        "\n",
        "```python\n",
        "def sigmoidPrime(self, s):  #derivative of sigmoid  \n",
        "    return s * (1 - s)\n",
        "```\n",
        "\n",
        "<img src=\"sigmoid_derivative.png\"  width=\"25%\" alt=\"weights\">\n",
        "\n",
        "Here is an illustration of what the derivative of the sigmoid function looks like. The value is very small when you get far from 0 and gets larger only close to 0. Basically, when the neural network is very sure about a certain neuron, we do not want to change the value of that neuron and passing the value of the neuron through the sigmoid derivative will help with that. On the other hand, if the neural network is not as sure about the neuron, we want to change it more. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Putting it All Together\n",
        "Let's implement our backward propagation function by using the method of gradient descent we just discovered.\n",
        "\n",
        "First, we will find the error in our function by taking the difference of our output layer (o) and the actual value (y). Next we need to figure out how much to change the output layer, so we calculate this delta by multiplying the error of the output layer with the derivative of the sigmoid function. Luckily, we've already defined a function for this.\n",
        "\n",
        "Once we've figured out the delta output sum for o, we go back to the hidden layer, z2, and calculate its error by taking the dot product of our o_delta and the transpose of the weights we used on it, W2. The reason we use the transpose of the second set of weights is so that we can apply the error of the output to each weight. Remember that o and W2 are 3x1 matrices; in order to do multiplication via the dot product W2 is transposed so the resulting matrix for z2_error is 3x3.\n",
        "\n",
        "Next, we do the same thing as we did with the output error and multiply the error by the derivative of the sigmoid function to figure out the change in z2.\n",
        "\n",
        "Now, we adjust our weights accordingly. Let's adjust the first set of weights by dotting our input with the change in the hidden layer. We take the transpose again to make the multiplication possible. Then we can combine the result with the first set of weights, element-wise (thanks Numpy!). We do the same thing to the second set of weights except we use the hidden layer and the output layer to do so.\n",
        "\n",
        "```python\n",
        "def backward(self, X, y, o):  # backward propagate through the network  \n",
        "    self.o_error = y - o # error in output  \n",
        "    self.o_delta = self.o_error*self.sigmoidPrime(o) # applying derivative of sigmoid to error  \n",
        "    self.z2_error = self.o_delta.dot(self.W2.T) # z2 error: how much our hidden layer weights contributed to output error  \n",
        "    self.z2_delta = self.z2_error*self.sigmoidPrime(self.z2) # applying derivative of sigmoid to z2 error  \n",
        "    self.W1 += X.T.dot(self.z2_delta) # adjusting first set (input --> hidden) weights  \n",
        "    self.W2 += self.z2.T.dot(self.o_delta) # adjusting second set (hidden --> output) weights\n",
        "```    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining Loss\n",
        "Here's a quick one. We've already defined the for loop to run our neural network a thousand times. Fill in the calculation for the loss function below! Take the mean of the square of the difference between the predicted and the actual output.\n",
        "\n",
        "```python\n",
        "nn = neural_network()\n",
        "for i in range(1000): # trains the nn 1,000 times\n",
        "  print(\"Input: \\n\" + str(X))\n",
        "  print(\"Actual Output: \\n\" + str(y))\n",
        "  print(\"Predicted Output: \\n\" + str(nn.forward(X)))\n",
        "  print(\"Loss: \\n\" + str(np.mean(np.square(y - nn.forward(X))))) # mean squared error\n",
        "  print(\"\\n\")\n",
        "  nn.train(X, y)\n",
        "```  \n",
        "\n",
        "Why take the square? Some of the errors will be negative. So, if we averaged the errors without squaring we might get close to 0 when the real loss is much larger. This Loss value is simply a way to quantify how far we are from the 'perfect' neural network.\n",
        "\n",
        "### Predict Function\n",
        "Now, let’s create a new function that prints our predicted output for x_predicted. All we have to run is forward(x_predicted) to return an output!\n",
        "\n",
        "Let's write a predict member function within our class that prints out the input x_predicted matrix and the output matrix after it is passed into the forward() function.\n",
        "\n",
        "```python\n",
        "def predict(self):\n",
        "  print(\"Predicted data based on trained weights: \")\n",
        "  print(\"Input (scaled): \\n\" + str(x_predicted))\n",
        "  print(\"Output: \\n\" + str(self.forward(x_predicted)))\n",
        "```  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Overfitting\n",
        "Here’s what we got after training the network 150,000 times. \n",
        "Keep in mind that doing this is grossly overfitting our data: \n",
        "training to essentially memorize the testing set. \n",
        "Models like these aren't very useful for making predictions on data that does not originate \n",
        "from the dataset.\n",
        "\n",
        "```\n",
        "# 150000\n",
        "\n",
        "Input (scaled):\n",
        "[[0.4 0.9]\n",
        " [0.2 0.5]\n",
        " [0.6 0.6]]\n",
        "Actual Output:\n",
        "[[0.92]\n",
        " [0.86]\n",
        " [0.89]]\n",
        "Predicted Output:\n",
        "[[0.92]\n",
        " [0.86]\n",
        " [0.89]]\n",
        "Loss:\n",
        "5.904467817735647e-17\n",
        "\n",
        "Predicted data based on trained weights:\n",
        "Input (scaled):\n",
        "[[1. 1.]]\n",
        "Output:\n",
        "[[0.93545994]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9nBzkgdbjcA"
      },
      "source": [
        "\n",
        "\n",
        "#### That's it. Congratulations! \n",
        "\n",
        "You have just learned how to create and train a neural network from scratch using PyTorch. There are so many things you can do with the shallow network we have just implemented. You can add more hidden layers or try to incorporate the bias terms for practice. \n",
        "\n",
        "### This is the plain code with just NUMPY, no PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# X = (hours studying, hours sleeping), y = score on test\n",
        "xAll = np.array(([2, 9], [1, 5], [3, 6], [5, 10]), dtype=float) # input data\n",
        "y = np.array(([92], [86], [89]), dtype=float) # output\n",
        "\n",
        "# scale units\n",
        "xAll = xAll/np.amax(xAll, axis=0) # scaling input data\n",
        "y = y/100 # scaling output data (max test score is 100)\n",
        "\n",
        "# split data\n",
        "X = np.split(xAll, [3])[0] # training data\n",
        "xPredicted = np.split(xAll, [3])[1] # testing data\n",
        "\n",
        "y = np.array(([92], [86], [89]), dtype=float)\n",
        "y = y/100 # max test score is 100\n",
        "\n",
        "class Neural_Network(object):\n",
        "  def __init__(self):\n",
        "  #parameters\n",
        "    self.inputSize = 2\n",
        "    self.outputSize = 1\n",
        "    self.hiddenSize = 3\n",
        "\n",
        "  #weights\n",
        "    self.W1 = np.random.randn(self.inputSize, self.hiddenSize) # (3x2) weight matrix from input to hidden layer\n",
        "    self.W2 = np.random.randn(self.hiddenSize, self.outputSize) # (3x1) weight matrix from hidden to output layer\n",
        "\n",
        "  def forward(self, X):\n",
        "    #forward propagation through our network\n",
        "    self.z = np.dot(X, self.W1) # dot product of X (input) and first set of 3x2 weights\n",
        "    self.z2 = self.sigmoid(self.z) # activation function\n",
        "    self.z3 = np.dot(self.z2, self.W2) # dot product of hidden layer (z2) and second set of 3x1 weights\n",
        "    o = self.sigmoid(self.z3) # final activation function\n",
        "    return o\n",
        "\n",
        "  def sigmoid(self, s):\n",
        "    # activation function\n",
        "    return 1/(1+np.exp(-s))\n",
        "\n",
        "  def sigmoidPrime(self, s):\n",
        "    #derivative of sigmoid\n",
        "    return s * (1 - s)\n",
        "\n",
        "  def backward(self, X, y, o):\n",
        "    # backward propagate through the network\n",
        "    self.o_error = y - o # error in output\n",
        "    self.o_delta = self.o_error*self.sigmoidPrime(o) # applying derivative of sigmoid to error\n",
        "\n",
        "    self.z2_error = self.o_delta.dot(self.W2.T) # z2 error: how much our hidden layer weights contributed to output error\n",
        "    self.z2_delta = self.z2_error*self.sigmoidPrime(self.z2) # applying derivative of sigmoid to z2 error\n",
        "\n",
        "    self.W1 += X.T.dot(self.z2_delta) # adjusting first set (input --> hidden) weights\n",
        "    self.W2 += self.z2.T.dot(self.o_delta) # adjusting second set (hidden --> output) weights\n",
        "\n",
        "  def train(self, X, y):\n",
        "    o = self.forward(X)\n",
        "    self.backward(X, y, o)\n",
        "\n",
        "  def saveWeights(self):\n",
        "    np.savetxt(\"w1.txt\", self.W1, fmt=\"%s\")\n",
        "    np.savetxt(\"w2.txt\", self.W2, fmt=\"%s\")\n",
        "\n",
        "  def predict(self):\n",
        "    print (\"Predicted data based on trained weights: \")\n",
        "    print (\"Input (scaled): \\n\" + str(xPredicted))\n",
        "    print (\"Output: \\n\" + str(self.forward(xPredicted)))\n",
        "\n",
        "NN = Neural_Network()\n",
        "for i in range(1000): # trains the NN 1,000 times\n",
        "  print (\"# \" + str(i) + \"\\n\")\n",
        "  print (\"Input (scaled): \\n\" + str(X))\n",
        "  print (\"Actual Output: \\n\" + str(y))\n",
        "  print (\"Predicted Output: \\n\" + str(NN.forward(X)))\n",
        "  print (\"Loss: \\n\" + str(np.mean(np.square(y - NN.forward(X))))) # mean sum squared loss\n",
        "  print (\"\\n\")\n",
        "  NN.train(X, y)\n",
        "\n",
        "NN.saveWeights()\n",
        "NN.predict()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcms4BCySKXj"
      },
      "source": [
        "## References:\n",
        "- [PyTorch nn. Modules](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-custom-nn-modules)\n",
        "- [Build a Neural Network with Numpy](https://enlight.nyc/neural-network)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
