{"cells":[{"cell_type":"markdown","source":["# Chapter 2: Transfer Learning\n","## Step-by-Step Guide to Adding a New Speech Command\n","1. Access the model on Google Drive\n","2. Load the Pretrained Model\n","If you already have a saved model from the micro speech notebook, load it. This command loads the entire model (architecture (.pb) and weights (variables) from the directory where your saved_model.pb resides."],"metadata":{"id":"1jSQGCDpS2gz"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748909629812,"user_tz":420,"elapsed":20301,"user":{"displayName":"Dr. Juergen Kienhoefer","userId":"06255351862773127066"}},"outputId":"19b05a2a-e9f1-4d9d-a1fe-5abdf62d8c49","id":"dtfsSl6_2zNP"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!saved_model_cli show --dir /content/drive/MyDrive/Notebooks/models --tag_set serve --signature_def serving_default\n"],"metadata":{"id":"ehOZACTvwYGG","executionInfo":{"status":"ok","timestamp":1748909648315,"user_tz":420,"elapsed":15891,"user":{"displayName":"Dr. Juergen Kienhoefer","userId":"06255351862773127066"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c6eb6f5f-ad8c-46af-bd2e-47132dbb8f6f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-06-03 00:13:54.786350: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1748909634.896227     752 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1748909634.972235     752 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-06-03 00:13:55.116294: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-06-03 00:14:04.098041: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","The given SavedModel SignatureDef contains the following input(s):\n","  inputs['input'] tensor_info:\n","      dtype: DT_FLOAT\n","      shape: (1, 1960)\n","      name: Reshape_1:0\n","The given SavedModel SignatureDef contains the following output(s):\n","  outputs['output'] tensor_info:\n","      dtype: DT_FLOAT\n","      shape: (1, 12)\n","      name: labels_softmax:0\n","Method name is: tensorflow/serving/predict\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","# Enable eager execution, needed for Transfer Learning\n","tf.compat.v1.disable_eager_execution()  # Disable first (sometimes helps)\n","tf.compat.v1.enable_eager_execution()   # Force enable\n","# Check if it's enabled\n","print(\"Eager execution enabled:\", tf.executing_eagerly())\n","\n","import numpy as np\n","import torch\n","\n","# Load your pretrained model\n","# Load the TFLite model\n","model_path = \"/content/drive/MyDrive/Colab/models/float_model.tflite\"\n","pretrained_model = tf.lite.Interpreter(model_path=model_path)\n","pretrained_model.allocate_tensors()\n","\n","# Get input and output details\n","input_details = pretrained_model.get_input_details()\n","print(\"Input Shape:\", input_details[0]['shape'])\n","output_details = pretrained_model.get_output_details()\n","\n","# Print summary\n","print(\"Model Summary:\")\n","print(\"Input Shape:\", input_details[0]['shape'])\n","print(\"Input Type:\", input_details[0]['dtype'])\n","print(\"Output Shape:\", output_details[0]['shape'])\n","print(\"Output Type:\", output_details[0]['dtype'])\n","\n","# Create a dummy input with the correct shape and type\n","dummy_input = np.random.randint(-128, 127, size=(1, 1960), dtype=np.int8)\n","\n","# load the .pb model for transfer training\n","pb_model = \"/content/drive/MyDrive/Colab/models/saved_model\"\n","# Load the model using TFSMLayer\n","keras_model = tf.keras.layers.TFSMLayer(pb_model, call_endpoint='serving_default')\n","\n"],"metadata":{"id":"aD_FQbUBTCMZ","executionInfo":{"status":"ok","timestamp":1740271714134,"user_tz":480,"elapsed":15351,"user":{"displayName":"Juergen Kienhoefer","userId":"01745980657165839611"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"154939c9-fa8a-4147-f3ba-8dd8f4d07091"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Eager execution enabled: True\n","Input Shape: [   1 1960]\n","Model Summary:\n","Input Shape: [   1 1960]\n","Input Type: <class 'numpy.float32'>\n","Output Shape: [1 4]\n","Output Type: <class 'numpy.float32'>\n"]}]},{"cell_type":"markdown","source":["3. Freeze existing layer and modify the Final Layer for Transfer Learning\n","Assuming the last layer is a Dense layer for classification, you'll replace it with a new Dense layer that has one additional class.\n","\n","For transfer learning:\n","\n","- Ensure your preprocessing pipeline produces int8 quantized features of length 1960.\n","\n","- If you're adding a new word, you'll likely need to modify the final layer(s) of the model to accommodate the new class.\n","\n","- When fine-tuning, make sure to maintain the input quantization scheme.\n","\n","To understand the 1960-length input better:\n","\n","- It could represent 20ms frames of audio, with each frame producing\n","\n","**98 features (20 * 98 = 1960)**\n","\n","\n","You should investigate the preprocessing steps used in the original model training to replicate them for your new data."],"metadata":{"id":"9Cik5c3SULY_"}},{"cell_type":"code","source":["# Load the SavedModel\n","pretrained_model = tf.saved_model.load(pb_model)\n","print(pretrained_model.signatures)\n","\n","# Extract the inference function\n","infer = pretrained_model.signatures[\"serving_default\"]\n","\n","## Get input shape dynamically\n","input_shape = infer.structured_input_signature[1][\"input\"].shape[1:]\n","\n","# Define a Keras wrapper for the SavedModel inference function\n","class SavedModelWrapper(tf.keras.layers.Layer):\n","    def __init__(self, infer_function):\n","        super(SavedModelWrapper, self).__init__()\n","        self.infer_function = infer_function\n","\n","    def call(self, inputs):\n","        return self.infer_function(input=inputs)[\"output\"]\n","\n","# Create a new input layer\n","input_tensor = tf.keras.Input(shape=input_shape, dtype=tf.float32, name=\"input\")\n","\n","# Pass the input through the wrapped SavedModel layer\n","wrapped_model = SavedModelWrapper(infer)\n","pretrained_output = wrapped_model(input_tensor)\n","print(pretrained_output.shape)\n","\n","# Freeze the original model's weights\n","for var in pretrained_model.variables:\n","    var._trainable = False\n","\n","# Add a new dense output layer for \"computer\" class\n","new_output = tf.keras.layers.Dense(5, activation=\"softmax\", name=\"new_output\")(pretrained_output)\n","new_label_map_index = 5\n","\n","# Create a new Keras model\n","transfer_model = tf.keras.Model(inputs=input_tensor, outputs=new_output)\n","\n","# Compile for training\n","transfer_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","# Show model summary\n","transfer_model.summary()"],"metadata":{"id":"qB1hHALcUPVj","executionInfo":{"status":"ok","timestamp":1740273951645,"user_tz":480,"elapsed":124,"user":{"displayName":"Juergen Kienhoefer","userId":"01745980657165839611"}},"colab":{"base_uri":"https://localhost:8080/","height":276},"outputId":"eda57004-ba03-42c6-9415-5d342887ddd0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["_SignatureMap({'serving_default': <ConcreteFunction () -> Dict[['output', TensorSpec(shape=(1, 4), dtype=tf.float32, name=None)]] at 0x7C1568459ED0>})\n","(1, 4)\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional_1\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ input (\u001b[38;5;33mInputLayer\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1960\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ saved_model_wrapper_1                │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4\u001b[0m)                      │               \u001b[38;5;34m0\u001b[0m │\n","│ (\u001b[38;5;33mSavedModelWrapper\u001b[0m)                  │                             │                 │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ new_output (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m5\u001b[0m)                      │              \u001b[38;5;34m25\u001b[0m │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1960</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ saved_model_wrapper_1                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                      │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SavedModelWrapper</span>)                  │                             │                 │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ new_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span> │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25\u001b[0m (100.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25</span> (100.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25\u001b[0m (100.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25</span> (100.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"markdown","source":["4. Prepare Your New Data\n","Ensure you have new training data for the additional command, along with a mix of existing data (to avoid forgetting the old commands). Preprocess this data in the same way as during the original model training.\n","\n","```\n","dataset/\n","\n","├── computer/\n","│   ├── sample1.wav\n","│   ├── sample2.wav\n","│   └── ...\n","├── stop/\n","│   ├── sample1.wav\n","│   ├── sample2.wav\n","│   └── ...\n","```\n","\n","\n"],"metadata":{"id":"CA_zJ8GtVQyu"}},{"cell_type":"code","source":["!pip install librosa scipy"],"metadata":{"id":"a7TQCc_jdGAl","executionInfo":{"status":"ok","timestamp":1740271717840,"user_tz":480,"elapsed":3502,"user":{"displayName":"Juergen Kienhoefer","userId":"01745980657165839611"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7ae65ce7-56c5-4bf6-f3a4-f9bc6ad62cef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.10.2.post1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n","Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n","Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.26.4)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n","Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n","Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.61.0)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n","Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n","Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n","Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.12.2)\n","Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n","Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy-loader>=0.1->librosa) (24.2)\n","Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.44.0)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n"]}]},{"cell_type":"markdown","source":["##Prepare the Dataset for training on the new data\n","\n","1. Constants & Setup:\n","\n","Defines constants such as sample rate (16kHz), window size (1960 samples), and stride (480 samples per window).\n","Specifies the dataset path (DATASET_PATH) where audio files are stored.\n","Sets the output file path (OUTPUT_PATH) for the processed dataset.\n","2. Audio Processing Functions:\n","\n","load_and_preprocess_audio(file_path):\n","\n","Loads a .wav file using librosa.\n","Ensures all audio clips have a fixed length (1 second / 16,000 samples).\n","Trims excess samples if too long or pads if too short.\n","Normalizes audio to [-1,1] range.\n","extract_windows(audio, label, stride, window_size):\n","\n","Slides a window of 1960 samples over the 16,000-sample audio.\n","Extracts overlapping windows with a stride of 480 samples.\n","Assigns the same label to each extracted window.\n","3. Processing the Entire Dataset:\n","\n","Iterates through subdirectories in audio_dataset, treating each directory name as a label.\n","Assigns a numeric label (label_map) to each word category.\n","For each .wav file:\n","Loads and processes the audio.\n","Extracts overlapping windows and assigns labels.\n","Stores extracted windows and labels in lists.\n","4. Saving the Processed Data:\n","\n","Converts collected windows (X) and labels (y) into NumPy arrays.\n","Saves them into processed_data.npz along with the label_map for later training."],"metadata":{"id":"ixaGSgxAdExK"}},{"cell_type":"code","source":["import os\n","import librosa\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","\n","# Constants\n","SAMPLE_RATE = 16000  # Ensure all audio files are at 16kHz\n","AUDIO_LENGTH = 16000  # 1-second clips (16,000 samples)\n","WINDOW_SIZE = 1960  # Model's input size\n","STRIDE = 480  # 30ms stride for sliding window\n","\n","DATASET_PATH = \"/content/drive/MyDrive/Colab/audio_dataset\"  # Update if needed\n","OUTPUT_PATH = \"processed_data.npz\"\n","\n","def load_and_preprocess_audio(file_path):\n","    \"\"\"Loads an audio file, normalizes, and ensures fixed length.\"\"\"\n","    audio, sr = librosa.load(file_path, sr=SAMPLE_RATE)  # Load file\n","    if len(audio) > AUDIO_LENGTH:\n","        audio = audio[:AUDIO_LENGTH]  # Trim if too long\n","    else:\n","        audio = np.pad(audio, (0, AUDIO_LENGTH - len(audio)), 'constant')  # Pad if too short\n","\n","    # Normalize audio to [-1, 1] range\n","    audio = audio / np.max(np.abs(audio)) if np.max(np.abs(audio)) > 0 else audio\n","    return audio\n","\n","def extract_windows(audio, label, stride=STRIDE, window_size=WINDOW_SIZE):\n","    \"\"\"Splits 16,000-sample audio into overlapping 1960-sample windows.\"\"\"\n","    windows = []\n","    labels = []\n","\n","    for start in range(0, len(audio) - window_size, stride):  # Slide over audio\n","        window = audio[start:start + window_size]\n","        windows.append(window)\n","        labels.append(label)  # Assign label based on whether keyword is inside\n","\n","    return np.array(windows), np.array(labels)\n","\n","# Step 1: Load dataset and process audio files\n","all_windows = []\n","all_labels = []\n","label_map = {'computer': 4}  # Assign label 0 to \"computer\" (update for other words as needed)\n","\n","for label in sorted(os.listdir(DATASET_PATH)):\n","    class_path = os.path.join(DATASET_PATH, label)\n","    if os.path.isdir(class_path) and label == \"computer\":  # Only process \"computer\"\n","        for file_name in os.listdir(class_path):\n","            if file_name.lower().endswith(\".wav\"):\n","                file_path = os.path.join(class_path, file_name)\n","                audio = load_and_preprocess_audio(file_path)\n","                windows, labels = extract_windows(audio, label_map[label])\n","                all_windows.append(windows)\n","                all_labels.append(labels)\n","\n","# Step 2: Convert to NumPy arrays\n","X = np.concatenate(all_windows, axis=0)\n","y = np.concatenate(all_labels, axis=0)\n","\n","# Step 3: Save processed data\n","np.savez(OUTPUT_PATH, X=X, y=y, label_map=label_map)\n","\n","print(f\"Processed data saved as {OUTPUT_PATH}\")\n","print(f\"Label map: {label_map}\")"],"metadata":{"id":"Nja5cewjdYMB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740273994519,"user_tz":480,"elapsed":136,"user":{"displayName":"Juergen Kienhoefer","userId":"01745980657165839611"}},"outputId":"a5addd9c-6b0f-4fc8-a2d2-3736ee0dd357"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processed data saved as processed_data.npz\n","Label map: {'computer': 4}\n"]}]},{"cell_type":"markdown","source":["It will save processed_data.npz, which contains:\n","\n","- X_train, y_train: Training data\n","- X_val, y_val: Validation data\n","- X_test, y_test: Test data\n","- labels: Class mappings\n","\n","Processed Dataset can be used for training."],"metadata":{"id":"8Uk-put-d_fH"}},{"cell_type":"markdown","source":["*5*. Load the new dataset"],"metadata":{"id":"jXWrToLTWLvl"}},{"cell_type":"code","source":["\n","label_map = {}\n","\n","# Load the processed dataset\n","data = np.load(\"processed_data.npz\", allow_pickle=True)\n","print(\"Unique labels in saved dataset:\", np.unique(data[\"y\"]))\n","print(\"Stored label_map in processed_data.npz:\", data[\"label_map\"].item())\n","\n","# Extract the data and labels\n","X = data[\"X\"]\n","y = data[\"y\"]\n","label_map = data[\"label_map\"].item()  # Ensure it's a dictionary\n","\n","print(f\"Loaded data: {X.shape}, {y.shape}\")\n","print(f\"Label map: {label_map}\")\n","\n","# Step 2: Split the data manually into train, validation, and test sets\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n","\n","print(f\"Train set: {X_train.shape}, {y_train.shape}\")\n","print(f\"Validation set: {X_val.shape}, {y_val.shape}\")\n","print(f\"Test set: {X_test.shape}, {y_test.shape}\")\n","\n","# Step 3: Optionally normalize or reshape the data as required by your model\n","# Example: Reshaping for Conv2D input if needed\n","X_train = X_train.reshape((-1, X_train.shape[1], 1))  # Reshape to (batch_size, time_steps, features, 1)\n","X_val = X_val.reshape((-1, X_val.shape[1], 1))\n","X_test = X_test.reshape((-1, X_test.shape[1], 1))\n","\n","# Example: Normalize data (if needed)\n","X_train = X_train / np.max(np.abs(X_train), axis=1, keepdims=True)\n","X_val = X_val / np.max(np.abs(X_val), axis=1, keepdims=True)\n","X_test = X_test / np.max(np.abs(X_test), axis=1, keepdims=True)\n","\n","# Now you're ready to train your model!\n","# model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10)\n"],"metadata":{"id":"7kWifecXVW8f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740274013946,"user_tz":480,"elapsed":16,"user":{"displayName":"Juergen Kienhoefer","userId":"01745980657165839611"}},"outputId":"dd708741-48ea-4bfe-8653-80097413404c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Unique labels in saved dataset: [4]\n","Stored label_map in processed_data.npz: {'computer': 4}\n","Loaded data: (570, 1960), (570,)\n","Label map: {'computer': 4}\n","Train set: (456, 1960), (456,)\n","Validation set: (57, 1960), (57,)\n","Test set: (57, 1960), (57,)\n"]}]},{"cell_type":"markdown","source":["##Adjusting the One-Hot Encoding:\n","1. Ensure correct class indices: After modifying the model's output to have 5 units, ensure that y_train, y_val, and y_test contain labels from the correct range. Since your label map is {'computer': 1}, you might need to map your current dataset to match this.\n","\n","2. Add Dummy Classes (Optional): If your dataset currently only has \"computer\" and no other labels (like \"yes\" and \"no\"), you might need to simulate those other classes for the training process (or adapt the architecture further for transfer learning).\n","\n","3. One-Hot Encoding: Assuming you have 5 possible classes now, use one-hot encoding as shown before:"],"metadata":{"id":"MsCNTa2HhJsK"}},{"cell_type":"code","source":["from tensorflow.keras.utils import to_categorical\n","\n","# One-hot encode the labels for 5 classes\n","num_classes = np.max(y_train) + 1  # Set the correct number of classes\n","print(f\"Number of classes: {num_classes}\")  # Debugging print\n","print(\"Unique labels in y_train:\", np.unique(y_train))\n","print(\"Max label:\", np.max(y_train))\n","\n","y_train_cat = to_categorical(y_train, num_classes=num_classes)\n","y_val_cat = to_categorical(y_val, num_classes=num_classes)\n","y_test_cat = to_categorical(y_test, num_classes=num_classes)\n","\n","# Reverse the dictionary to map numbers to words\n","label_map_reverse = {v: k for k, v in label_map.items()}\n","\n","# Print the unique labels in y_train as words\n","unique_labels = np.unique(y_train)  # Get unique numbers in y_train\n","\n","print(\"Label map:\", label_map)  # Check the original mapping of labels\n","print(\"Reversed Label Map:\", label_map_reverse)  # Check if index 9 exists\n","\n","print(\"Unique labels in y_train:\", np.unique(y_train))  # Check actual labels in y_train\n","print(\"Max label:\", np.max(y_train))  # Double-check if max label is within range"],"metadata":{"id":"NTkDaVlkha0g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740274019380,"user_tz":480,"elapsed":13,"user":{"displayName":"Juergen Kienhoefer","userId":"01745980657165839611"}},"outputId":"cd987e2f-fb5a-46ee-ad9b-313f7d42c314"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of classes: 5\n","Unique labels in y_train: [4]\n","Max label: 4\n","Label map: {'computer': 4}\n","Reversed Label Map: {4: 'computer'}\n","Unique labels in y_train: [4]\n","Max label: 4\n"]}]},{"cell_type":"markdown","source":["##Training with Your Transfer Model\n","You can then proceed to train the model with the one-hot encoded labels"],"metadata":{"id":"yDBeCfSxhfVj"}},{"cell_type":"code","source":["print(type(X_train))\n","print(type(y_train_cat))\n","\n","print(\"Eager execution enabled:\", tf.executing_eagerly())\n","\n","history = transfer_model.fit(\n","    X_train, y_train_cat,\n","    validation_data=(X_val, y_val_cat),\n","    epochs=10,\n","    batch_size=32\n",")"],"metadata":{"id":"pYkiDwVHhWUM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740274025983,"user_tz":480,"elapsed":3816,"user":{"displayName":"Juergen Kienhoefer","userId":"01745980657165839611"}},"outputId":"836cc8ff-ca38-4f4a-f9bc-9f84c499ffe4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'numpy.ndarray'>\n","<class 'numpy.ndarray'>\n","Eager execution enabled: True\n","Epoch 1/10\n","\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.0000e+00 - loss: 45.7004 - val_accuracy: 0.0000e+00 - val_loss: 40.6239\n","Epoch 2/10\n","\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0000e+00 - loss: 44.2599 - val_accuracy: 0.0000e+00 - val_loss: 39.3605\n","Epoch 3/10\n","\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 9.0419e-04 - loss: 42.8765 - val_accuracy: 0.0351 - val_loss: 38.1229\n","Epoch 4/10\n","\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.3823 - loss: 41.5847 - val_accuracy: 0.9298 - val_loss: 36.9118\n","Epoch 5/10\n","\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9827 - loss: 40.1987 - val_accuracy: 1.0000 - val_loss: 35.7287\n","Epoch 6/10\n","\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 38.9324 - val_accuracy: 1.0000 - val_loss: 34.5738\n","Epoch 7/10\n","\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 37.6612 - val_accuracy: 1.0000 - val_loss: 33.4482\n","Epoch 8/10\n","\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 36.4629 - val_accuracy: 1.0000 - val_loss: 32.3519\n","Epoch 9/10\n","\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 35.2216 - val_accuracy: 1.0000 - val_loss: 31.2853\n","Epoch 10/10\n","\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 34.0519 - val_accuracy: 1.0000 - val_loss: 30.2485\n"]}]},{"cell_type":"markdown","source":["7. Convert & Optimize for Deployment\n","\n","After fine-tuning, convert your model to TensorFlow Lite and apply quantization:"],"metadata":{"id":"TxBLgsdCVe69"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","transfer_model.export(\"custom_keyword_spotter_finetuned\")\n","\n","# Convert to TensorFlow Lite\n","converter = tf.lite.TFLiteConverter.from_saved_model(\"custom_keyword_spotter_finetuned\")\n","tflite_model = converter.convert()\n","\n","# Save the TFLite model\n","with open(\"custom_keyword_spotter_finetuned.tflite\", \"wb\") as f:\n","    f.write(tflite_model)\n","\n","print(\"Fine-tuned model saved as custom_keyword_spotter_finetuned.tflite\")\n"],"metadata":{"id":"G_eMHKp7VTmG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740274039572,"user_tz":480,"elapsed":1105,"user":{"displayName":"Juergen Kienhoefer","userId":"01745980657165839611"}},"outputId":"0b2accc0-8505-4c37-b5f6-f053c54a2b92"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved artifact at 'custom_keyword_spotter_finetuned'. The following endpoints are available:\n","\n","* Endpoint 'serve'\n","  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 1960), dtype=tf.float32, name='input')\n","Output Type:\n","  TensorSpec(shape=(1, 5), dtype=tf.float32, name=None)\n","Captures:\n","  136431388531536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  136431388530768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  136431388534032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  136431388534224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  136431388532688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  136431388538064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","Fine-tuned model saved as custom_keyword_spotter_finetuned.tflite\n"]}]},{"cell_type":"markdown","source":["8. Convert and Quantize the Model"],"metadata":{"id":"HBVMO_yYiIjJ"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","\n","# Enable default quantization optimizations\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Enable default quantization\n","converter.inference_input_type = tf.int8\n","converter.inference_output_type = tf.int8\n","\n","# Define a representative dataset generator\n","def representative_dataset_gen():\n","    for i in range(100):\n","        # Replace this with your real dataset loading function\n","        sample = np.random.rand(1, 1960).astype(np.float32)  # Adjust the shape as needed\n","        yield [sample]\n","\n","# Set the representative dataset for calibration during quantization\n","converter.representative_dataset = representative_dataset_gen\n","\n","# Convert and save the quantized model\n","quantized_tflite_model = converter.convert()\n","\n","with open(\"custom_keyword_spotter_finetuned_quantized.tflite\", \"wb\") as f:\n","    f.write(quantized_tflite_model)\n","\n","print(\"Quantized model saved as custom_keyword_spotter_finetuned_quantized.tflite\")"],"metadata":{"id":"t5PaGIBJiN-P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740274045465,"user_tz":480,"elapsed":582,"user":{"displayName":"Juergen Kienhoefer","userId":"01745980657165839611"}},"outputId":"df02eb79-555e-4dad-a31d-cdd9ac493bfa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Quantized model saved as custom_keyword_spotter_finetuned_quantized.tflite\n"]}]},{"cell_type":"markdown","source":["## Testing the Transfer Learned model's accuracy\n","\n","---\n","\n","\n","\n","Verify that the model we've exported is still accurate, using the TF Lite Python API and our test set."],"metadata":{"id":"sxu4OAZ1uHKg"}},{"cell_type":"code","source":["# Helper function to run inference\n","def run_tflite_inference(tflite_model_path, model_type=\"Float\"):\n","  # Load test data\n","  np.random.seed(0) # set random seed for reproducible test results.\n","  with tf.compat.v1.Session() as sess:\n","    test_data, test_labels = audio_processor.get_data(\n","        -1, 0, model_settings, BACKGROUND_FREQUENCY, BACKGROUND_VOLUME_RANGE,\n","        TIME_SHIFT_MS, 'testing', sess)\n","  test_data = np.expand_dims(test_data, axis=1).astype(np.float32)\n","\n","  # Initialize the interpreter\n","  interpreter = tf.lite.Interpreter(tflite_model_path,\n","                                    experimental_op_resolver_type=tf.lite.experimental.OpResolverType.BUILTIN_REF)\n","  interpreter.allocate_tensors()\n","\n","  input_details = interpreter.get_input_details()[0]\n","  output_details = interpreter.get_output_details()[0]\n","\n","  # For quantized models, manually quantize the input data from float to integer\n","  if model_type == \"Quantized\":\n","    input_scale, input_zero_point = input_details[\"quantization\"]\n","    test_data = test_data / input_scale + input_zero_point\n","    test_data = test_data.astype(input_details[\"dtype\"])\n","\n","  correct_predictions = 0\n","  for i in range(len(test_data)):\n","    interpreter.set_tensor(input_details[\"index\"], test_data[i])\n","    interpreter.invoke()\n","    output = interpreter.get_tensor(output_details[\"index\"])[0]\n","    top_prediction = output.argmax()\n","    correct_predictions += (top_prediction == test_labels[i])\n","\n","  print('%s model accuracy is %f%% (Number of test samples=%d)' % (\n","      model_type, (correct_predictions * 100) / len(test_data), len(test_data)))"],"metadata":{"id":"2U6QQ2RJuPJA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Clone the TensorFlow Github Repository, which contains the relevant code required to run this test."],"metadata":{"id":"kcOFqJBm1WXv"}},{"cell_type":"code","source":["!git clone -q --depth 1 https://github.com/tensorflow/tensorflow"],"metadata":{"id":"jSjrk5WZ1VfJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","# We add this path so we can import the speech processing modules.\n","sys.path.append(\"/content/tensorflow/tensorflow/examples/speech_commands/\")\n","import input_data\n","import models\n","\n","LOGS_DIR = 'logs/'\n","SAMPLE_RATE = 16000\n","CLIP_DURATION_MS = 1000\n","WINDOW_SIZE_MS = 30.0\n","WINDOW_STRIDE = 20\n","FEATURE_BIN_COUNT = 40\n","PREPROCESS = 'micro'\n","SILENT_PERCENTAGE = 10\n","UNKNOWN_PERCENTAGE = 10\n","FEATURE_BIN_COUNT = 40\n","BACKGROUND_FREQUENCY = 0.8\n","BACKGROUND_VOLUME_RANGE = 0.1\n","TIME_SHIFT_MS = 100.0\n","\n","\n","#DATA_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n","#DATASET_DIR =  'dataset/'\n","\n","DATA_URL = \"\"\n","DATASET_DIR = '/content/drive/MyDrive/Colab/audio_dataset'  # Update if needed\n","\n","\n","VALIDATION_PERCENTAGE = 10\n","TESTING_PERCENTAGE = 10\n","\n","# 5. Combine the original and new labels to create the full list of `WANTED_WORDS`\n","WANTED_WORDS = WANTED_WORDS = \"yes,no,computer\"\n","\n","# Prepare model settings\n","model_settings = models.prepare_model_settings(\n","    len(input_data.prepare_words_list(WANTED_WORDS.split(','))),\n","    SAMPLE_RATE, CLIP_DURATION_MS, WINDOW_SIZE_MS,\n","    WINDOW_STRIDE, FEATURE_BIN_COUNT, PREPROCESS)\n","\n","# Set up audio processor\n","audio_processor = input_data.AudioProcessor(\n","    DATA_URL, DATASET_DIR,\n","    SILENT_PERCENTAGE, UNKNOWN_PERCENTAGE,\n","    WANTED_WORDS.split(','), VALIDATION_PERCENTAGE,\n","    TESTING_PERCENTAGE, model_settings, LOGS_DIR)\n","\n","# Now, you can run inference with your prepared model and audio processor\n","run_tflite_inference(\"custom_keyword_spotter_finetuned.tflite\", model_type=\"Float\")\n","\n","\n","# Test with a float model\n","run_tflite_inference(\"custom_keyword_spotter_finetuned.tflite\", model_type=\"Float\")\n","\n","# Test with a quantized model (if you have one)\n","run_tflite_inference(\"custom_keyword_spotter_finetuned_quantized.tflite\", model_type=\"Quantized\")\n"],"metadata":{"id":"sGDOc_PvuWrr","colab":{"base_uri":"https://localhost:8080/","height":411},"executionInfo":{"status":"error","timestamp":1740274144190,"user_tz":480,"elapsed":22700,"user":{"displayName":"Juergen Kienhoefer","userId":"01745980657165839611"}},"outputId":"be083a43-3af0-4a35-dbf8-b84c65f25004"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['/content', '/env/python', '/usr/lib/python311.zip', '/usr/lib/python3.11', '/usr/lib/python3.11/lib-dynload', '', '/usr/local/lib/python3.11/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.11/dist-packages/IPython/extensions', '/root/.ipython', '/usr/local/lib/python3.11/dist-packages/setuptools/_vendor', '/content/tensorflow/tensorflow/examples/speech_commands/', '/content/tensorflow/tensorflow/examples/speech_commands/']\n"]},{"output_type":"error","ename":"ValueError","evalue":"Background sample is too short! Need more than 16000 samples but only 16000 were found","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-45-fae4dc5667ea>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# Now, you can run inference with your prepared model and audio processor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mrun_tflite_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"custom_keyword_spotter_finetuned.tflite\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Float\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-42-819e65447471>\u001b[0m in \u001b[0;36mrun_tflite_inference\u001b[0;34m(tflite_model_path, model_type)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# set random seed for reproducible test results.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     test_data, test_labels = audio_processor.get_data(\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_settings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBACKGROUND_FREQUENCY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBACKGROUND_VOLUME_RANGE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         TIME_SHIFT_MS, 'testing', sess)\n","\u001b[0;32m/content/tensorflow/tensorflow/examples/speech_commands/input_data.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, how_many, offset, model_settings, background_frequency, background_volume_range, time_shift, mode, sess)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0mbackground_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackground_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbackground_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackground_samples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmodel_settings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'desired_samples'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m           raise ValueError(\n\u001b[0m\u001b[1;32m    574\u001b[0m               \u001b[0;34m'Background sample is too short! Need more than %d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m               \u001b[0;34m' samples but only %d were found'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Background sample is too short! Need more than 16000 samples but only 16000 were found"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"1DP324XE1bss8AnQJ1fTXBew1JzzVkPxA","timestamp":1740161828380},{"file_id":"1PpiIr_PquZPTX9pPy-xOs993K76p_PES","timestamp":1740008029849},{"file_id":"https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/train/train_micro_speech_model.ipynb","timestamp":1739823641435}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.9.13"},"vscode":{"interpreter":{"hash":"22cb1d09959a40fdc50ccd77b5464bb60602aea13b58d7f13d7eaffcd0bc7c7d"}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}