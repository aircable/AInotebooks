{"cells":[{"cell_type":"markdown","id":"0c44cd59","metadata":{"lines_to_next_cell":0,"id":"0c44cd59"},"source":["# XOR Neural Network in PyTorch and Keras\n","\n","## Introduction\n","This script demonstrates how to implement a simple neural network for solving the XOR problem using **PyTorch** and **Keras**. The XOR problem is a classic example of a problem that cannot be solved with a simple perceptron but requires a multi-layer neural network. We will:\n","\n","1. Implement the XOR neural network in **PyTorch**.\n","2. Implement the same XOR neural network in **Keras**.\n","3. Train both models on XOR data.\n","4. Convert the trained Keras model to **TensorFlow Lite (TFLite)** for deployment on embedded systems like the **ESP32S3**.\n","\n","## Understanding the Tools\n","\n","### PyTorch\n","- PyTorch is an open-source machine learning framework developed by Facebook AI.\n","- It is widely used for deep learning research and production.\n","- It provides dynamic computation graphs, making it flexible and easy to debug.\n","\n","### TensorFlow / Keras\n","- TensorFlow is a machine learning framework developed by Google.\n","- Keras is a high-level API built on top of TensorFlow that simplifies the process of building neural networks.\n","- TensorFlow Lite (TFLite) is a lightweight version of TensorFlow designed for mobile and embedded devices like **ESP32S3**.\n","\n","---\n","\n","## Step 1: Import Required Libraries"]},{"cell_type":"code","execution_count":1,"id":"59bbdcc3","metadata":{"lines_to_next_cell":0,"id":"59bbdcc3","executionInfo":{"status":"ok","timestamp":1749073225145,"user_tz":420,"elapsed":22979,"user":{"displayName":"Dr. Juergen Kienhoefer","userId":"06255351862773127066"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras"]},{"cell_type":"markdown","id":"058292b0","metadata":{"lines_to_next_cell":0,"id":"058292b0"},"source":["**Explanation:**\n","- `torch` is the main PyTorch module.\n","- `torch.nn` contains classes for building neural networks.\n","- `torch.optim` provides optimization algorithms.\n","- `numpy` is used for numerical operations.\n","- `tensorflow` and `keras` are used to implement the model in Keras.\n","\n","---\n","\n","## Step 2: Define the XOR Dataset"]},{"cell_type":"code","execution_count":2,"id":"943e9367","metadata":{"lines_to_next_cell":0,"id":"943e9367","executionInfo":{"status":"ok","timestamp":1749073225164,"user_tz":420,"elapsed":4,"user":{"displayName":"Dr. Juergen Kienhoefer","userId":"06255351862773127066"}}},"outputs":[],"source":["# XOR input and expected output\n","data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n","labels = np.array([[0], [1], [1], [0]], dtype=np.float32)"]},{"cell_type":"markdown","id":"aceb2efe","metadata":{"lines_to_next_cell":0,"id":"aceb2efe"},"source":["**Explanation:**\n","- The XOR dataset consists of four binary input pairs and their expected outputs.\n","- The goal of the network is to learn the XOR function.\n","\n","---\n","\n","## Step 3: Implement the XOR Neural Network in PyTorch"]},{"cell_type":"code","execution_count":3,"id":"6c205a8d","metadata":{"lines_to_next_cell":0,"id":"6c205a8d","executionInfo":{"status":"ok","timestamp":1749073225166,"user_tz":420,"elapsed":4,"user":{"displayName":"Dr. Juergen Kienhoefer","userId":"06255351862773127066"}}},"outputs":[],"source":["class XOR_NN_PyTorch(nn.Module):\n","    def __init__(self):\n","        super(XOR_NN_PyTorch, self).__init__()\n","        self.hidden = nn.Linear(2, 3)\n","        self.output = nn.Linear(3, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.sigmoid(self.hidden(x))\n","        x = self.sigmoid(self.output(x))\n","        return x"]},{"cell_type":"markdown","id":"88ecc78a","metadata":{"lines_to_next_cell":0,"id":"88ecc78a"},"source":["**Explanation:**\n","- `nn.Linear(2, 3)`: A fully connected layer with **2 input neurons** and **3 hidden neurons**.\n","- `nn.Linear(3, 1)`: A second fully connected layer with **3 hidden neurons** and **1 output neuron**.\n","- `Sigmoid()`: Activation function that maps outputs to values between 0 and 1.\n","\n","---\n","\n","## Step 4: Train the PyTorch Model\n","**Choosing the Optimizer (How the Model Learns)**\n","\n","Why not **SGD** (Stochastic Gradient Descent)?\n","\n","- XOR is a non-linearly separable problem, so SGD struggles.\n","- It would need 30,000+ epochs to get decent results.\n","\n","Why **Adam** (Adaptive Moment Estimation)?\n","\n","- Adaptive learning rate: It adjusts how fast it learns at different times.\n","- Momentum-based: Helps avoid getting stuck in bad spots.\n","- Handles sparse gradients well â€“ Useful for deep networks.\n","- Faster training: Works well for most deep learning tasks.\n","- Works out of the box â€“ Requires less tuning than SGD.\n","\n","Why **RMSprop** (Root Mean Square Propagation)?\n","\n","- Good for small datasets.\n","- Helps stabilize learning but can sometimes learn too fast and overfit.\n","- Adapts the learning rate for each weight in the network by dividing the gradient by a moving average of its recent magnitudes"]},{"cell_type":"code","execution_count":4,"id":"df8554d7","metadata":{"lines_to_next_cell":0,"colab":{"base_uri":"https://localhost:8080/"},"id":"df8554d7","executionInfo":{"status":"ok","timestamp":1749073242030,"user_tz":420,"elapsed":16863,"user":{"displayName":"Dr. Juergen Kienhoefer","userId":"06255351862773127066"}},"outputId":"4f7147aa-62a0-4058-8585-47e3900a4c5c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Loss: 0.254387\n","Epoch 1000, Loss: 0.000083\n","Epoch 2000, Loss: 0.000024\n","Epoch 3000, Loss: 0.000011\n","Epoch 4000, Loss: 0.000005\n","Epoch 5000, Loss: 0.000003\n","Epoch 6000, Loss: 0.000002\n","Epoch 7000, Loss: 0.000001\n","Epoch 8000, Loss: 0.000001\n","Epoch 9000, Loss: 0.000000\n","\n","âœ… Model trained and saved as xor_demo.pth\n"]}],"source":["model = XOR_NN_PyTorch()\n","criterion = nn.MSELoss()\n","# XOR is not linearly separable, and SGD struggles with non-convex optimization problems\n","#optimizer = optim.SGD(model.parameters(), lr=0.1) # needs 30000 epochs\n","# adaptive optimizer, adjusts the learning rate dynamically\n","optimizer = optim.Adam(model.parameters(), lr=0.1)  # ğŸ”¥ Faster Convergence\n","# or RMSprop â†’ âš¡ Works well with small datasets\n","#optimizer = optim.RMSprop( model.parameters(), lr=0.1) # a bit too good\n","\n","# PyTorch models work with tensors instead of normal lists/arrays.\n","# convert the input data (data) and expected labels (labels) into PyTorch tensors so the model can process them\n","data_tensor = torch.tensor(data)\n","labels_tensor = torch.tensor(labels)\n","\n","for epoch in range(10000):\n","    optimizer.zero_grad()      # Reset gradients to zero\n","    output = model(data_tensor)  # Forward pass: Get model's prediction\n","    loss = criterion(output, labels_tensor)  # Compute the loss\n","    loss.backward()            # Compute gradients (how to adjust weights)\n","    optimizer.step()           # Update model weights based on gradients\n","\n","    if epoch % 1000 == 0:\n","        print(f\"Epoch {epoch}, Loss: {loss.item():.6f}\")\n","\n","# âœ… Save the Trained Model\n","torch.save(model.state_dict(), \"xor_demo.pth\")\n","print(\"\\nâœ… Model trained and saved as xor_demo.pth\")\n"]},{"cell_type":"markdown","id":"e670e968","metadata":{"id":"e670e968"},"source":["**What this code does**\n","### The Training Loop (How Learning Happens)\n","\n","| Step | Code | Explanation |\n","| ----- | ----- | ----- |\n","| **1** | `optimizer.zero_grad()` | Clears gradients from the previous step to avoid accumulation. |\n","| **2** | `output = model(data_tensor)` | Feeds input data through the network (forward pass). |\n","| **3** | `loss = criterion(output, labels_tensor)` | Computes how far the prediction is from the correct label. |\n","| **4** | `loss.backward()` | Calculates gradients (derivatives) for each weight in the model. |\n","| **5** | `optimizer.step()` | Updates model weights using the computed gradients. |\n","| **6** | `if epoch % 1000 == 0:` | Every 1000 epochs, prints the current loss. |\n","\n"]},{"cell_type":"markdown","id":"b5c7216b","metadata":{"id":"b5c7216b"},"source":["### Discussion Questions\n","\n","- What would happen if we used SGD instead of Adam?\n","- Why do we use a hidden layer in this model?\n","- How could we reduce training time further?\n","- How do we know if the model is overfitting?\n","\n"]},{"cell_type":"markdown","id":"6d7df3b1","metadata":{"id":"6d7df3b1"},"source":["**What would happen if we used SGD instead of Adam?**\n","\n","- Slower Learning: SGD updates weights at a fixed learning rate, which is not ideal for XOR (a non-linearly separable problem).\n","- More Epochs Required: Would need 30,000+ epochs to reach good accuracy.\n","- Gets Stuck Easily: Can get trapped in local minima because it lacks adaptive learning rates.\n","- When is SGD Useful?: Works well for large datasets but struggles with small ones like XOR.\n","\n","ğŸ‘‰ Follow-up Questions:\n","\n","- Why does Adam perform better here?\n","- When should we prefer SGD over Adam?\n","- What about RMSprop?\n","\n","2ï¸âƒ£ Why do we use a hidden layer in this model?\n","\n","- XOR is Not Linearly Separable: A simple linear classifier (like logistic regression) cannot learn XOR.\n","- Captures Nonlinear Patterns: The hidden layer allows the model to map inputs to a curved decision boundary.\n","- More Neurons â‰  Always Better: Too many neurons may lead to overfitting.\n","\n","ğŸ‘‰ **Follow-up Questions:**\n","\n","- What happens if we remove the hidden layer?\n","- Could a single hidden neuron solve XOR?\n","- What if we add more hidden layers?\n","\n","3ï¸âƒ£ How could we reduce training time further?\n","\n","- Use a smaller number of epochs (e.g., 10,000 instead of 30,000) â€“ does it still learn XOR?\n","- Reduce learning rate to prevent overshooting.\n","- Try different optimizers like RMSprop or AdamW.\n","- Batch Training: Instead of using the full dataset at once, could we train on mini-batches?\n","- Use GPU acceleration (PyTorch can run on CUDA).\n","\n","ğŸ‘‰ **Follow-up Questions:**\n","\n","- Does reducing epochs harm accuracy?\n","- Would adding more neurons make training faster or slower?\n","- Could different activation functions (e.g., ReLU instead of Sigmoid) help?\n","\n","4ï¸âƒ£ How do we know if the model is overfitting?\n","\n","- Loss is very low, but generalization is bad (i.e., works well on training data but fails on new data).\n","- Too many parameters in a small dataset can lead to overfitting.\n","- Look at Validation Loss: If training loss is low but validation loss is high, it's overfitting.\n","- Try Regularization (L2 weight decay) or dropout layers to prevent it.\n","\n","ğŸ‘‰ **Follow-up Questions:**\n","\n","- If we train for even more epochs (e.g., 100,000), what happens?\n","- Would increasing the dataset size help reduce overfitting?\n","- Could we use data augmentation for an XOR problem?\n"]},{"cell_type":"markdown","id":"8be30f55","metadata":{"id":"8be30f55"},"source":["### Prediction\n","We are testing the model to see how it works"]},{"cell_type":"code","execution_count":5,"id":"4bf8f72b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4bf8f72b","executionInfo":{"status":"ok","timestamp":1749073242053,"user_tz":420,"elapsed":17,"user":{"displayName":"Dr. Juergen Kienhoefer","userId":"06255351862773127066"}},"outputId":"a07bc6f7-b16c-4a55-c28f-6943ec87cd91"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Neural Network Predictions:\n","Inputs: [0. 0.] -> Prediction: 0.0004 -> Rounded: 0\n","Inputs: [0. 1.] -> Prediction: 0.9996 -> Rounded: 1\n","Inputs: [1. 0.] -> Prediction: 0.9994 -> Rounded: 1\n","Inputs: [1. 1.] -> Prediction: 0.0004 -> Rounded: 0\n"]}],"source":["import torch\n","\n","# Ensure the model is in evaluation mode (disables dropout, batch norm updates)\n","model.eval()\n","\n","# Convert input data to a PyTorch tensor\n","data_tensor = torch.tensor(data, dtype=torch.float32)\n","\n","# Perform predictions (no_grad prevents tracking gradients for efficiency)\n","with torch.no_grad():\n","    predictions = model(data_tensor)\n","\n","# Print predictions\n","print(\"\\nNeural Network Predictions:\")\n","for i, pred in enumerate(predictions):\n","    pred_value = pred.item()  # Convert tensor to Python float\n","    print(f\"Inputs: {data[i]} -> Prediction: {pred_value:.4f} -> Rounded: {round(pred_value)}\")\n"]},{"cell_type":"markdown","id":"93a5345e","metadata":{"id":"93a5345e"},"source":["---\n","\n","## BONUS: Implement the XOR Neural Network in Keras\n","\n","This is informational only. We are not using Keras for building models for embedded systems\n","\n","PyTorch is more relevant\n","- âœ… PyTorch helps understand embedded AI better\n","- âœ… Easier transition to Edge AI tools (ESP capability, flexibility, optimization)\n","- âœ… Keras adds unnecessary abstraction"]},{"cell_type":"markdown","id":"fbcb68c9","metadata":{"id":"fbcb68c9"},"source":["```!python\n","keras_model = keras.Sequential([\n","    keras.layers.Dense(3, activation='sigmoid', input_shape=(2,)),\n","    keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","keras_model.compile(optimizer='sgd', loss='mean_squared_error')\n","keras_model.fit(data, labels, epochs=10000, verbose=0)\n","```\n"]},{"cell_type":"markdown","id":"6a8967e5","metadata":{"id":"6a8967e5"},"source":["**Explanation:**\n","- `Sequential()`: Creates a simple feedforward neural network.\n","- `Dense(3, activation='sigmoid')`: Hidden layer with 3 neurons and sigmoid activation.\n","- `Dense(1, activation='sigmoid')`: Output layer with 1 neuron and sigmoid activation.\n","- `compile()`: Configures the model with **SGD** optimizer and **MSE** loss function.\n","- `fit()`: Trains the model for **10,000 epochs**."]},{"cell_type":"markdown","id":"c9deb95d","metadata":{"id":"c9deb95d"},"source":["## **Step 5:** Convert PyTorch Model to ONNX\n","\n","First, we need to convert the trained PyTorch model into ONNX format, which acts as a bridge between PyTorch and TensorFlow.\n","\n","It could be done with command line:\n","```\n","onnx2tf -i xor_model.onnx -o xor_model_tf -cotof\n","```"]},{"cell_type":"code","source":["!pip install onnx"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NnrpFZCZU07c","executionInfo":{"status":"ok","timestamp":1749073249791,"user_tz":420,"elapsed":7739,"user":{"displayName":"Dr. Juergen Kienhoefer","userId":"06255351862773127066"}},"outputId":"2293f457-f9a5-47c0-faf4-2c1e69ae0536"},"id":"NnrpFZCZU07c","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting onnx\n","  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n","Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n","Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.5)\n","Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.13.2)\n","Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: onnx\n","Successfully installed onnx-1.18.0\n"]}]},{"cell_type":"code","execution_count":7,"id":"152cc21f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"152cc21f","executionInfo":{"status":"ok","timestamp":1749073249980,"user_tz":420,"elapsed":190,"user":{"displayName":"Dr. Juergen Kienhoefer","userId":"06255351862773127066"}},"outputId":"2a819f3f-4247-425f-91c4-efb1687a042f"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… ONNX model saved to xor_model.onnx\n"]}],"source":["import torch\n","import onnx\n","\n","# Load your trained PyTorch model\n","model = XOR_NN_PyTorch()  # Replace with your model class\n","model.load_state_dict(torch.load(\"xor_demo.pth\", weights_only=True))\n","model.eval()\n","\n","# Define input shape\n","dummy_input = torch.randn(1, 2)  # Same shape as your model input\n","# dummy_input = torch.tensor([[0.0, 0.0]], dtype=torch.float32) # or like that, start with zero\n","\n","# Convert to ONNX\n","onnx_path = \"xor_model.onnx\"\n","torch.onnx.export(\n","    model,  # Use the original PyTorch model, not traced_model\n","    dummy_input,\n","    onnx_path,\n","    input_names=[\"input\"],\n","    output_names=[\"output\"],\n","    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n","    opset_version=17  # Use a recent ONNX opset version\n",")\n","\n","print(f\"âœ… ONNX model saved to {onnx_path}\")"]},{"cell_type":"markdown","id":"35cc40f9","metadata":{"id":"35cc40f9"},"source":["## **Step 6:** Convert ONNX to TensorFlow\n","\n","We convert ONNX to TensorFlow using onnx-tf.\n","\n","Install the ONNX-TensorFlow converter, the depencencies are a bit tricky"]},{"cell_type":"code","execution_count":8,"id":"8b5424cd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8b5424cd","executionInfo":{"status":"ok","timestamp":1749073268764,"user_tz":420,"elapsed":18783,"user":{"displayName":"Dr. Juergen Kienhoefer","userId":"06255351862773127066"}},"outputId":"6e8bbdfd-ba3a-487f-d22d-99549a4035b6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting onnx2tf\n","  Downloading onnx2tf-1.27.10-py3-none-any.whl.metadata (149 kB)\n","\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/149.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”\u001b[0m \u001b[32m143.4/149.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m149.2/149.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting onnx-graphsurgeon\n","  Downloading onnx_graphsurgeon-0.5.8-py2.py3-none-any.whl.metadata (8.2 kB)\n","Collecting ai_edge_litert\n","  Downloading ai_edge_litert-1.3.0-cp311-cp311-manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n","Collecting sng4onnx\n","  Downloading sng4onnx-1.0.4-py3-none-any.whl.metadata (4.6 kB)\n","Collecting onnx-simplifier\n","  Downloading onnx_simplifier-0.4.36-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n","Collecting onnxruntime\n","  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n","Collecting sne4onnx\n","  Downloading sne4onnx-1.0.13-py3-none-any.whl.metadata (7.0 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from onnx-graphsurgeon) (2.0.2)\n","Requirement already satisfied: onnx>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from onnx-graphsurgeon) (1.18.0)\n","Collecting backports.strenum (from ai_edge_litert)\n","  Downloading backports_strenum-1.2.8-py3-none-any.whl.metadata (3.6 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from ai_edge_litert) (25.2.10)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from ai_edge_litert) (4.67.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from ai_edge_litert) (4.13.2)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from onnx-simplifier) (13.9.4)\n","Collecting coloredlogs (from onnxruntime)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (5.29.5)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->onnx-simplifier) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->onnx-simplifier) (2.19.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->onnx-simplifier) (0.1.2)\n","Downloading onnx2tf-1.27.10-py3-none-any.whl (447 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m448.0/448.0 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnx_graphsurgeon-0.5.8-py2.py3-none-any.whl (57 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ai_edge_litert-1.3.0-cp311-cp311-manylinux_2_17_x86_64.whl (11.2 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sng4onnx-1.0.4-py3-none-any.whl (5.9 kB)\n","Downloading onnx_simplifier-0.4.36-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sne4onnx-1.0.13-py3-none-any.whl (7.2 kB)\n","Downloading backports_strenum-1.2.8-py3-none-any.whl (7.9 kB)\n","Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sng4onnx, sne4onnx, onnx2tf, humanfriendly, backports.strenum, onnx-graphsurgeon, coloredlogs, ai_edge_litert, onnxruntime, onnx-simplifier\n","Successfully installed ai_edge_litert-1.3.0 backports.strenum-1.2.8 coloredlogs-15.0.1 humanfriendly-10.0 onnx-graphsurgeon-0.5.8 onnx-simplifier-0.4.36 onnx2tf-1.27.10 onnxruntime-1.22.0 sne4onnx-1.0.13 sng4onnx-1.0.4\n"]}],"source":["!pip install onnx2tf onnx-graphsurgeon ai_edge_litert sng4onnx onnx-simplifier onnxruntime sne4onnx"]},{"cell_type":"markdown","id":"b6805352","metadata":{"id":"b6805352"},"source":["### Run the conversion"]},{"cell_type":"code","execution_count":9,"id":"21f2f6b2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"21f2f6b2","executionInfo":{"status":"ok","timestamp":1749073278267,"user_tz":420,"elapsed":9494,"user":{"displayName":"Dr. Juergen Kienhoefer","userId":"06255351862773127066"}},"outputId":"0c69954f-5fe4-4a6f-c746-b4b3d879552a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\u001b[07mModel optimizing started\u001b[0m ============================================================\n","Simplifying...\n","Finish! Here is the difference:\n","â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n","â”ƒ            â”ƒ Original Model â”ƒ Simplified Model â”ƒ\n","â”¡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n","â”‚ Constant   â”‚ 4              â”‚ 4                â”‚\n","â”‚ Gemm       â”‚ 2              â”‚ 2                â”‚\n","â”‚ Sigmoid    â”‚ 2              â”‚ 2                â”‚\n","â”‚ Model Size â”‚ 645.0B         â”‚ 802.0B           â”‚\n","â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","\n","Simplifying...\n","Finish! Here is the difference:\n","â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n","â”ƒ            â”ƒ Original Model â”ƒ Simplified Model â”ƒ\n","â”¡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n","â”‚ Constant   â”‚ 4              â”‚ 4                â”‚\n","â”‚ Gemm       â”‚ 2              â”‚ 2                â”‚\n","â”‚ Sigmoid    â”‚ 2              â”‚ 2                â”‚\n","â”‚ Model Size â”‚ 802.0B         â”‚ 802.0B           â”‚\n","â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","\n","Simplifying...\n","Finish! Here is the difference:\n","â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n","â”ƒ            â”ƒ Original Model â”ƒ Simplified Model â”ƒ\n","â”¡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n","â”‚ Constant   â”‚ 4              â”‚ 4                â”‚\n","â”‚ Gemm       â”‚ 2              â”‚ 2                â”‚\n","â”‚ Sigmoid    â”‚ 2              â”‚ 2                â”‚\n","â”‚ Model Size â”‚ 802.0B         â”‚ 802.0B           â”‚\n","â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","\n","\u001b[32mModel optimizing complete!\u001b[0m\n","\n","\u001b[07mAutomatic generation of each OP name started\u001b[0m ========================================\n","\u001b[32mAutomatic generation of each OP name complete!\u001b[0m\n","\n","\u001b[07mModel loaded\u001b[0m ========================================================================\n","\n","\u001b[07mModel conversion started\u001b[0m ============================================================\n","\u001b[32mINFO:\u001b[0m \u001b[32minput_op_name\u001b[0m: input \u001b[32mshape\u001b[0m: ['batch_size', 2] \u001b[32mdtype\u001b[0m: float32\n","\n","\u001b[32mINFO:\u001b[0m \u001b[32m2 / 5\u001b[0m\n","\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: /hidden/Gemm\n","\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: input \u001b[36mshape\u001b[0m: ['batch_size', 2] \u001b[36mdtype\u001b[0m: float32\n","\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: hidden.weight \u001b[36mshape\u001b[0m: [3, 2] \u001b[36mdtype\u001b[0m: float32\n","\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: hidden.bias \u001b[36mshape\u001b[0m: [3] \u001b[36mdtype\u001b[0m: float32\n","\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: /hidden/Gemm_output_0 \u001b[36mshape\u001b[0m: ['batch_size', 3] \u001b[36mdtype\u001b[0m: float32\n","\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n","\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (None, 2) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n","\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (2, 3) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n","\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (3,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n","\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n","\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n","\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add/AddV2:0 \u001b[34mshape\u001b[0m: (None, 3) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n","\n","\u001b[32mINFO:\u001b[0m \u001b[32m3 / 5\u001b[0m\n","\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Sigmoid\u001b[35m onnx_op_name\u001b[0m: /sigmoid/Sigmoid\n","\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: /hidden/Gemm_output_0 \u001b[36mshape\u001b[0m: ['batch_size', 3] \u001b[36mdtype\u001b[0m: float32\n","\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: /sigmoid/Sigmoid_output_0 \u001b[36mshape\u001b[0m: ['batch_size', 3] \u001b[36mdtype\u001b[0m: float32\n","\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: sigmoid\n","\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add/AddV2:0 \u001b[34mshape\u001b[0m: (None, 3) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n","\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.sigmoid/Sigmoid:0 \u001b[34mshape\u001b[0m: (None, 3) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n","\n","\u001b[32mINFO:\u001b[0m \u001b[32m4 / 5\u001b[0m\n","\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: /output/Gemm\n","\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: /sigmoid/Sigmoid_output_0 \u001b[36mshape\u001b[0m: ['batch_size', 3] \u001b[36mdtype\u001b[0m: float32\n","\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: output.weight \u001b[36mshape\u001b[0m: [1, 3] \u001b[36mdtype\u001b[0m: float32\n","\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: output.bias \u001b[36mshape\u001b[0m: [1] \u001b[36mdtype\u001b[0m: float32\n","\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: /output/Gemm_output_0 \u001b[36mshape\u001b[0m: ['batch_size', 1] \u001b[36mdtype\u001b[0m: float32\n","\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n","\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (None, 3) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n","\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (3, 1) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n","\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (1,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n","\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n","\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n","\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_1/AddV2:0 \u001b[34mshape\u001b[0m: (None, 1) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n","\n","\u001b[32mINFO:\u001b[0m \u001b[32m5 / 5\u001b[0m\n","\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Sigmoid\u001b[35m onnx_op_name\u001b[0m: /sigmoid_1/Sigmoid\n","\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: /output/Gemm_output_0 \u001b[36mshape\u001b[0m: ['batch_size', 1] \u001b[36mdtype\u001b[0m: float32\n","\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: output \u001b[36mshape\u001b[0m: ['batch_size', 1] \u001b[36mdtype\u001b[0m: float32\n","\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: sigmoid\n","\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add_1/AddV2:0 \u001b[34mshape\u001b[0m: (None, 1) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n","\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.sigmoid_1/Sigmoid:0 \u001b[34mshape\u001b[0m: (None, 1) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n","\n","\u001b[07msaved_model output started\u001b[0m ==========================================================\n","\u001b[32msaved_model output complete!\u001b[0m\n","\u001b[33mWARNING:\u001b[0m If you want tflite input OP name and output OP name to match ONNX input and output names, convert them after installing \"flatc\". Also, do not use symbols such as slashes in input/output OP names. To install flatc, run the following command:\n","wget https://github.com/PINTO0309/onnx2tf/releases/download/1.16.31/flatc.tar.gz && tar -zxvf flatc.tar.gz && sudo chmod +x flatc && sudo mv flatc /usr/bin/\n","\u001b[32mFloat32 tflite output complete!\u001b[0m\n","\u001b[33mWARNING:\u001b[0m If you want tflite input OP name and output OP name to match ONNX input and output names, convert them after installing \"flatc\". Also, do not use symbols such as slashes in input/output OP names. To install flatc, run the following command:\n","wget https://github.com/PINTO0309/onnx2tf/releases/download/1.16.31/flatc.tar.gz && tar -zxvf flatc.tar.gz && sudo chmod +x flatc && sudo mv flatc /usr/bin/\n","\u001b[32mFloat16 tflite output complete!\u001b[0m\n","Converted to TensorFlow SavedModel format.\n"]}],"source":["import onnx\n","import onnx2tf\n","\n","# Load ONNX model\n","onnx_model_path = \"xor_model.onnx\"\n","onnx_model = onnx.load(onnx_model_path)\n","\n","# Convert to TensorFlow\n","onnx2tf.convert(\n","    input_onnx_file_path=onnx_model_path,\n","    output_folder_path=\"xor_model\",\n","    copy_onnx_input_output_names_to_tflite=True  # Ensures input/output names are preserved\n",")\n","print(\"Converted to TensorFlow SavedModel format.\")"]},{"cell_type":"markdown","id":"6e12f323","metadata":{"id":"6e12f323"},"source":["### Prepare a Representative Dataset\n","\n","For INT8 quantization, TensorFlow Lite requires a representative dataset to calibrate the model's dynamic ranges. This dataset should be representative of the input data the model will process during inference."]},{"cell_type":"code","execution_count":10,"id":"d13c3ceb","metadata":{"id":"d13c3ceb","executionInfo":{"status":"ok","timestamp":1749073278282,"user_tz":420,"elapsed":11,"user":{"displayName":"Dr. Juergen Kienhoefer","userId":"06255351862773127066"}}},"outputs":[],"source":["import numpy as np\n","\n","def representative_dataset():\n","    for _ in range(100):\n","        # Provide data in the same shape as your model's input\n","        data = np.random.rand(1, 2).astype(np.float32)\n","        yield [data]\n"]},{"cell_type":"markdown","id":"6190d222","metadata":{"id":"6190d222"},"source":["Ensure that the data generated matches the input shape and data type expected by your model.\n","\n","### Convert to INT8 TFLite Model\n","\n","Use TensorFlow Lite's converter to quantize the model to INT8."]},{"cell_type":"code","execution_count":11,"id":"a5323ad9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a5323ad9","executionInfo":{"status":"ok","timestamp":1749073278388,"user_tz":420,"elapsed":94,"user":{"displayName":"Dr. Juergen Kienhoefer","userId":"06255351862773127066"}},"outputId":"139d3598-fabf-4c40-e4bd-26f8424b56a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Input details: [{'name': 'input', 'index': 0, 'shape': array([1, 2], dtype=int32), 'shape_signature': array([-1,  2], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n","Output details: [{'name': 'Identity', 'index': 12, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([-1,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"]}],"source":["import tensorflow as tf\n","\n","# Load the TFLite model\n","tflite_model_path = \"xor_model/xor_model_float32.tflite\"\n","interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n","\n","# Get model input details\n","input_details = interpreter.get_input_details()\n","output_details = interpreter.get_output_details()\n","\n","print(\"Input details:\", input_details)\n","print(\"Output details:\", output_details)\n"]},{"cell_type":"markdown","id":"a23e396c","metadata":{"id":"a23e396c"},"source":["### Quantization from Float32 to Int16 or Int8 for ESP32S3\n","\n","### Full Integer Quantization from Float32 (Recommended for ESP32S3)\n","\n","To improve performance on embedded hardware, we should quantize weights and activations:"]},{"cell_type":"code","execution_count":12,"id":"4284b740","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4284b740","executionInfo":{"status":"ok","timestamp":1749073279386,"user_tz":420,"elapsed":1000,"user":{"displayName":"Dr. Juergen Kienhoefer","userId":"06255351862773127066"}},"outputId":"d12d0b3f-14ed-44aa-aeb2-ff0167dd4e8d"},"outputs":[{"output_type":"stream","name":"stdout","text":["INT8 quantized TFLite model saved as 'xor_model_int8.tflite'.\n"]}],"source":["import tensorflow as tf\n","\n","# Load the SavedModel\n","converter = tf.lite.TFLiteConverter.from_saved_model(\"xor_model\")\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","converter.representative_dataset = representative_dataset\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n","converter.inference_input_type = tf.int8\n","converter.inference_output_type = tf.int8\n","\n","# Convert the model\n","tflite_quant_model = converter.convert()\n","\n","# Save the quantized model\n","with open(\"xor_model_int8.tflite\", \"wb\") as f:\n","    f.write(tflite_quant_model)\n","\n","print(\"INT8 quantized TFLite model saved as 'xor_model_int8.tflite'.\")\n"]},{"cell_type":"markdown","id":"427edd64","metadata":{"id":"427edd64"},"source":["### Verify TFLite Model Properties\n","After conversion, letâ€™s check if the model is properly quantized:"]},{"cell_type":"code","execution_count":13,"id":"e6c63eeb","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e6c63eeb","executionInfo":{"status":"ok","timestamp":1749073279413,"user_tz":420,"elapsed":26,"user":{"displayName":"Dr. Juergen Kienhoefer","userId":"06255351862773127066"}},"outputId":"dd87a3d3-f249-4bea-beef-5c483f01045b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Input details: [{'name': 'serving_default_input:0', 'index': 0, 'shape': array([1, 2], dtype=int32), 'shape_signature': array([-1,  2], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.0039170472882688046, -128), 'quantization_parameters': {'scales': array([0.00391705], dtype=float32), 'zero_points': array([-128], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n","Output details: [{'name': 'PartitionedCall:0', 'index': 12, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([-1,  1], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.00390625, -128), 'quantization_parameters': {'scales': array([0.00390625], dtype=float32), 'zero_points': array([-128], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"]}],"source":["interpreter = tf.lite.Interpreter(model_path=\"xor_model_int8.tflite\")\n","interpreter.allocate_tensors()\n","\n","input_details = interpreter.get_input_details()\n","output_details = interpreter.get_output_details()\n","\n","print(\"Input details:\", input_details)\n","print(\"Output details:\", output_details)\n"]},{"cell_type":"markdown","id":"06904283","metadata":{"id":"06904283"},"source":["Looks good! Your model has been successfully quantized to UINT8 (full integer quantization):\n","\n","âœ… Input dtype: numpy.uint8 (was float32)\n","âœ… Output dtype: numpy.uint8 (was float32)\n","âœ… Quantization scales:\n","\n","- Input: 0.0038925335\n","- Output: 0.00390625\n","\n","ğŸš€ XNNPACK delegate applied\n","\n","This means optimized execution using TensorFlow Lite's XNNPACK backend, which is great for performance on embedded devices!"]},{"cell_type":"markdown","id":"fd1460da","metadata":{"id":"fd1460da"},"source":["### Predictions\n","This section demonstrates how the trained neural network model makes predictions on XOR inputs. The goal of this demonstration is to showcase how a quantized neural network can accurately compute the XOR function while running efficiently on embedded hardware.\n","\n","In the previous implementation, the model was tested by feeding XOR input values and comparing the predicted outputs to expected results. Now, we will use the quantized TensorFlow Lite model and run inference using the TFLite interpreter.\n","\n","#### Code for Running Predictions\n","\n","The following code loads the quantized model and performs inference:"]},{"cell_type":"code","execution_count":14,"id":"0605c0be","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0605c0be","executionInfo":{"status":"ok","timestamp":1749073279447,"user_tz":420,"elapsed":30,"user":{"displayName":"Dr. Juergen Kienhoefer","userId":"06255351862773127066"}},"outputId":"847c3270-7b20-4155-9340-c16a2da61ed2"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Neural Network Predictions:\n","Inputs: [0. 0.] -> Prediction: 0.0000 -> Rounded: 0\n","Inputs: [0. 1.] -> Prediction: 0.9961 -> Rounded: 1\n","Inputs: [1. 0.] -> Prediction: 0.9961 -> Rounded: 1\n","Inputs: [1. 1.] -> Prediction: 0.0000 -> Rounded: 0\n"]}],"source":["import numpy as np\n","import tensorflow.lite as tflite\n","\n","# Load the TFLite model\n","interpreter = tflite.Interpreter(model_path=\"xor_model_int8.tflite\")\n","interpreter.allocate_tensors()\n","\n","# Get input and output details\n","input_details = interpreter.get_input_details()\n","output_details = interpreter.get_output_details()\n","\n","# Define XOR input samples\n","inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n","\n","# Quantize inputs\n","scale, zero_point = input_details[0]['quantization']\n","inputs_q = np.round(inputs / scale + zero_point).astype(np.int8)\n","\n","# Run inference\n","predictions = []\n","for x in inputs_q:\n","    x = np.expand_dims(x, axis=0)  # TFLite expects batch dimension\n","    interpreter.set_tensor(input_details[0]['index'], x)\n","    interpreter.invoke()\n","    output_data = interpreter.get_tensor(output_details[0]['index'])\n","\n","    # Dequantize output\n","    output_scale, output_zero_point = output_details[0]['quantization']\n","    pred = (output_data.astype(np.float32) - output_zero_point) * output_scale\n","\n","    predictions.append(pred[0][0])\n","\n","# Print results\n","print(\"\\nNeural Network Predictions:\")\n","for i, pred in enumerate(predictions):\n","    print(f\"Inputs: {inputs[i]} -> Prediction: {pred:.4f} -> Rounded: {round(pred)}\")\n"]},{"cell_type":"markdown","id":"810aebee","metadata":{"id":"810aebee"},"source":["Let's evaluate our work\n","\n","Measures Model File Sizes: Compares the storage requirements of the original PyTorch model and the quantized TFLite model.â€‹\n","Evaluates Inference Speed: Assesses the time taken to perform inference on a sample input for both models.â€‹\n","NVIDIA Developer Forums\n","Checks Prediction Consistency: Ensures that the quantized model's outputs are consistent with those of the original model.â€‹"]},{"cell_type":"code","execution_count":15,"id":"e632ed35","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e632ed35","executionInfo":{"status":"ok","timestamp":1749073279479,"user_tz":420,"elapsed":5,"user":{"displayName":"Dr. Juergen Kienhoefer","userId":"06255351862773127066"}},"outputId":"c36218b7-df2d-48ee-c46e-85efa37cd395"},"outputs":[{"output_type":"stream","name":"stdout","text":["Input: [[0. 0.]]\n","Output [[-128]], Dequantized [[0.]]\n","Input: [[0. 1.]]\n","Output [[127]], Dequantized [[0.99609375]]\n","Input: [[1. 0.]]\n","Output [[127]], Dequantized [[0.99609375]]\n","Input: [[1. 1.]]\n","Output [[-128]], Dequantized [[0.]]\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","\n","# Load the int8 TFLite model\n","interpreter_int8 = tf.lite.Interpreter(model_path='xor_model_int8.tflite')\n","interpreter_int8.allocate_tensors()\n","\n","# Retrieve input and output tensor details\n","input_details = interpreter_int8.get_input_details()\n","output_details = interpreter_int8.get_output_details()\n","\n","# Get quantization parameters for input and output\n","input_scale, input_zero_point = input_details[0]['quantization']\n","output_scale, output_zero_point = output_details[0]['quantization']\n","\n","# Prepare your float32 input data\n","input_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n","\n","# Iterate over each input\n","for single_input in input_data:\n","    # Reshape input to match model's expected input shape [1, 2]\n","    single_input = np.expand_dims(single_input, axis=0)\n","\n","    # Quantize the input data to int8\n","    input_data_int8 = np.round(single_input / input_scale + input_zero_point).astype(np.int8)\n","\n","    # Set the tensor with the quantized data\n","    interpreter_int8.set_tensor(input_details[0]['index'], input_data_int8)\n","\n","    # Run inference\n","    interpreter_int8.invoke()\n","\n","    # Retrieve and dequantize the output data\n","    output_data_int8 = interpreter_int8.get_tensor(output_details[0]['index'])\n","    output_data = (output_data_int8.astype(np.float32) - output_zero_point) * output_scale\n","\n","    print(\"Input:\", single_input)\n","    print(f\"Output {output_data_int8}, Dequantized {output_data}\")\n"]},{"cell_type":"markdown","id":"85058953","metadata":{"id":"85058953"},"source":["### Define Functions"]},{"cell_type":"code","execution_count":16,"id":"bf22088a","metadata":{"id":"bf22088a","executionInfo":{"status":"ok","timestamp":1749073279511,"user_tz":420,"elapsed":32,"user":{"displayName":"Dr. Juergen Kienhoefer","userId":"06255351862773127066"}}},"outputs":[],"source":["import os\n","import time\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import tensorflow as tf\n","\n","def get_file_size(file_path):\n","    \"\"\"Returns the size of the file in kilobytes.\"\"\"\n","    return os.path.getsize(file_path) / 1024\n","\n","def measure_inference_time(model, input_data, model_type='pytorch'):\n","    \"\"\"\n","    Measures the average inference time over the input data.\n","\n","    Parameters:\n","    - model: The model to evaluate.\n","    - input_data: Data to run through the model.\n","    - model_type: 'pytorch' or 'tflite' indicating the type of model.\n","\n","    Returns:\n","    - Average inference time in milliseconds.\n","    \"\"\"\n","    if model_type == 'pytorch':\n","        model.eval()\n","        with torch.no_grad():\n","            start_time = time.time()\n","            for _ in range(10):  # Run multiple times for averaging\n","                for single_input in input_data:\n","                    model(single_input)\n","            total_time = time.time() - start_time\n","    elif model_type == 'tflite':\n","        interpreter = tf.lite.Interpreter(model_content=model)\n","        interpreter.allocate_tensors()\n","        input_details = interpreter.get_input_details()\n","        output_details = interpreter.get_output_details()\n","\n","        start_time = time.time()\n","        for _ in range(10):  # Run multiple times for averaging\n","            for single_input in input_data:\n","                # Ensure the input is in the correct shape and type\n","                input_data = np.expand_dims(single_input, axis=0).astype(input_details[0]['dtype'])\n","                interpreter.set_tensor(input_details[0]['index'], input_data)\n","                interpreter.invoke()\n","                interpreter.get_tensor(output_details[0]['index'])\n","        total_time = time.time() - start_time\n","    else:\n","        raise ValueError(\"Unsupported model type. Choose 'pytorch' or 'tflite'.\")\n","\n","    avg_time = (total_time / (10 * len(input_data))) * 1000  # Convert to milliseconds\n","    return avg_time\n","\n","def evaluate_model_consistency(model, input_data, model_type='pytorch'):\n","    \"\"\"\n","    Evaluates the consistency of the model's outputs over the input data.\n","\n","    Parameters:\n","    - model_path: Path to the TFLite model file or PyTorch model.\n","    - input_data: Data to run through the model.\n","    - model_type: 'pytorch', 'tflite_float32', or 'tflite_int8' indicating the type of model.\n","\n","    Returns:\n","    - Standard deviation of the outputs.\n","    \"\"\"\n","    outputs = []\n","\n","    if model_type == 'pytorch':\n","        model.eval()\n","        print( input_data )\n","        with torch.no_grad():\n","            for single_input in input_data:\n","                single_input = torch.tensor(single_input, dtype=torch.float32).unsqueeze(0)  # Ensure shape (1,2)\n","                output = model(single_input)\n","                outputs.append(output.numpy().flatten())  # Flatten for consistency\n","\n","    elif model_type in ['tflite_float32', 'tflite_int8']:\n","        interpreter = tf.lite.Interpreter(model_content=model)\n","        interpreter.allocate_tensors()\n","        input_details = interpreter.get_input_details()\n","        output_details = interpreter.get_output_details()\n","\n","        # Retrieve input quantization parameters\n","        input_scale, input_zero_point = input_details[0]['quantization']\n","        output_scale, output_zero_point = output_details[0]['quantization']\n","\n","        for single_input in input_data:\n","            # Ensure input is a 2D array with shape [1, input_dim]\n","            single_input = np.expand_dims(single_input, axis=0).astype(np.float32)\n","\n","            if model_type == 'tflite_int8':\n","                # Quantize the input data for int8 model\n","                single_input = np.round(single_input / input_scale + input_zero_point).astype(np.int8)\n","\n","            # Set the tensor with the appropriately formatted data\n","            interpreter.set_tensor(input_details[0]['index'], single_input)\n","\n","            # Run inference\n","            interpreter.invoke()\n","\n","            # Retrieve and dequantize the output data\n","            output_data = interpreter.get_tensor(output_details[0]['index'])\n","            if model_type == 'tflite_int8':\n","                output_data = (output_data.astype(np.float32) - output_zero_point) * output_scale\n","\n","            outputs.append(output_data)\n","\n","    else:\n","        raise ValueError(\"Unsupported model type. Choose 'pytorch', 'tflite_float32', or 'tflite_int8'.\")\n","\n","    outputs = np.array(outputs)\n","    outputs = outputs.flatten()\n","    print( outputs )\n","\n","    expected_outputs = np.array([0, 1, 1, 0])\n","    print( expected_outputs )\n","    predictions = np.round(outputs)  # Round outputs to nearest binary value\n","    accuracy = np.mean(predictions == expected_outputs)\n","    return accuracy\n","\n"]},{"cell_type":"markdown","id":"3e297707","metadata":{"id":"3e297707"},"source":["### Load the PyTorch model"]},{"cell_type":"code","execution_count":18,"id":"5d769cc1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5d769cc1","executionInfo":{"status":"ok","timestamp":1749073722366,"user_tz":420,"elapsed":81,"user":{"displayName":"Dr. Juergen Kienhoefer","userId":"06255351862773127066"}},"outputId":"99169f0b-6dc0-4eb0-df1a-8f08f3e137ed"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["XOR_NN_PyTorch(\n","  (hidden): Linear(in_features=2, out_features=3, bias=True)\n","  (output): Linear(in_features=3, out_features=1, bias=True)\n","  (sigmoid): Sigmoid()\n",")"]},"metadata":{},"execution_count":18}],"source":["# Define the PyTorch model architecture\n","class XOR_NN_PyTorch(nn.Module):\n","    def __init__(self):\n","        super(XOR_NN_PyTorch, self).__init__()\n","        self.hidden = nn.Linear(2, 3)\n","        self.output = nn.Linear(3, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.sigmoid(self.hidden(x))\n","        x = self.sigmoid(self.output(x))\n","        return x\n","\n","# Load the model\n","pytorch_model_path = 'xor_demo.pth'\n","pytorch_model = XOR_NN_PyTorch()\n","pytorch_model.load_state_dict(torch.load(pytorch_model_path))\n","pytorch_model.eval()\n"]},{"cell_type":"markdown","id":"d395cfa3","metadata":{"id":"d395cfa3"},"source":["### Load the TFLite models"]},{"cell_type":"code","execution_count":19,"id":"c72a3468","metadata":{"id":"c72a3468","executionInfo":{"status":"ok","timestamp":1749073740996,"user_tz":420,"elapsed":8,"user":{"displayName":"Dr. Juergen Kienhoefer","userId":"06255351862773127066"}}},"outputs":[],"source":["# Paths to the TFLite models\n","tflite_float32_model_path = 'xor_model/xor_model_float32.tflite'\n","tflite_int8_model_path = 'xor_model_int8.tflite'\n","\n","# Load the TFLite models\n","with open(tflite_float32_model_path, 'rb') as f:\n","    tflite_float32_model = f.read()\n","\n","with open(tflite_int8_model_path, 'rb') as f:\n","    tflite_int8_model = f.read()\n"]},{"cell_type":"markdown","id":"734e8187","metadata":{"id":"734e8187"},"source":["### Prepare the input data for each model"]},{"cell_type":"code","execution_count":20,"id":"f12368e0","metadata":{"id":"f12368e0","executionInfo":{"status":"ok","timestamp":1749073755922,"user_tz":420,"elapsed":4,"user":{"displayName":"Dr. Juergen Kienhoefer","userId":"06255351862773127066"}}},"outputs":[],"source":["# XOR input data\n","xor_input_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n","\n","# For PyTorch: Convert to torch tensors\n","pytorch_input_data = [torch.tensor(data, dtype=torch.float32) for data in xor_input_data]\n"]},{"cell_type":"markdown","id":"3c769102","metadata":{"id":"3c769102"},"source":["### Evaluate the Models\n","\n","Measure the size, inference time, and consistency for each model:"]},{"cell_type":"code","execution_count":21,"id":"24a01cff","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"24a01cff","executionInfo":{"status":"ok","timestamp":1749073758907,"user_tz":420,"elapsed":79,"user":{"displayName":"Dr. Juergen Kienhoefer","userId":"06255351862773127066"}},"outputId":"8c5cb8c9-bbdf-40a0-c92b-4d29bceaf443"},"outputs":[{"output_type":"stream","name":"stdout","text":["[tensor([0., 0.]), tensor([0., 1.]), tensor([1., 0.]), tensor([1., 1.])]\n","[4.4463607e-04 9.9964261e-01 9.9940622e-01 4.2789589e-04]\n","[0 1 1 0]\n","[4.4463668e-04 9.9964261e-01 9.9940628e-01 4.2789482e-04]\n","[0 1 1 0]\n","[0.         0.99609375 0.99609375 0.        ]\n","[0 1 1 0]\n","PyTorch Model: Size = 2.0859375 bytes, Avg Inference Time = 0.142491 seconds\n","TFLite Float32 Model: Size = 2.09375 bytes, Avg Inference Time = 0.065184 seconds\n","TFLite INT8 Model: Size = 2.6953125 bytes, Avg Inference Time = 0.037932 seconds\n","Accuracy Check (Outputs):\n","PyTorch Model Output: 1.0\n","Model Accuracy: 100.00%\n","TFLite Float32 Model Output: 1.0\n","TFLite INT8 Model Output: 1.0\n"]}],"source":["# Evaluate PyTorch model\n","pytorch_size = get_file_size(pytorch_model_path)\n","pytorch_avg_time = measure_inference_time(pytorch_model, pytorch_input_data, 'pytorch')\n","pytorch_consistency = evaluate_model_consistency(pytorch_model, pytorch_input_data, 'pytorch')\n","\n","# Evaluate TFLite Float32 model\n","tflite_float32_size = get_file_size(tflite_float32_model_path)\n","tflite_float32_avg_time = measure_inference_time(tflite_float32_model, xor_input_data, 'tflite')\n","tflite_float32_consistency = evaluate_model_consistency(tflite_float32_model, xor_input_data, 'tflite_float32')\n","\n","# Evaluate TFLite INT8 model\n","tflite_int8_size = get_file_size(tflite_int8_model_path)\n","tflite_int8_avg_time = measure_inference_time(tflite_int8_model, xor_input_data, 'tflite')\n","tflite_int8_consistency = evaluate_model_consistency(tflite_int8_model, xor_input_data, 'tflite_int8')\n","\n","# Print results\n","print(f\"PyTorch Model: Size = {pytorch_size} bytes, Avg Inference Time = {pytorch_avg_time:.6f} seconds\")\n","print(f\"TFLite Float32 Model: Size = {tflite_float32_size} bytes, Avg Inference Time = {tflite_float32_avg_time:.6f} seconds\")\n","print(f\"TFLite INT8 Model: Size = {tflite_int8_size} bytes, Avg Inference Time = {tflite_int8_avg_time:.6f} seconds\")\n","\n","print(\"Accuracy Check (Outputs):\")\n","print(f\"PyTorch Model Output: {pytorch_consistency}\")\n","print(f\"Model Accuracy: {pytorch_consistency * 100:.2f}%\")\n","print(f\"TFLite Float32 Model Output: {tflite_float32_consistency}\")\n","print(f\"TFLite INT8 Model Output: {tflite_int8_consistency}\")"]},{"cell_type":"markdown","id":"ea4eb490","metadata":{"id":"ea4eb490"},"source":["### Interpretation\n","\n","- Typically, quantized models like the TFLite INT8 version are expected to have smaller sizes due to reduced precision. However, in your case, the INT8 model is larger. This anomaly might result from additional overhead introduced during the quantization process or specific model architecture characteristics.â€‹\n","\n","- The TFLite INT8 model demonstrates the fastest inference time, aligning with expectations that lower-precision models execute computations more swiftly. The PyTorch model exhibits the slowest inference, which is consistent with observations that TensorFlow Lite often outperforms PyTorch in mobile environments due to its optimized architecture.\n","\n","- The output consistency, as measured by standard deviation, indicates variability in model predictions. The PyTorch and TFLite Float32 models show some variability, while the TFLite INT8 model exhibits no variability (standard deviation of 0.0). This could suggest that the INT8 model is producing uniform outputs, potentially indicating a loss of sensitivity or dynamic range due to quantization. Such issues have been reported where INT8 quantization leads to degraded performance or incorrect confidence scores"]}],"metadata":{"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"kernelspec":{"display_name":"llm","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}